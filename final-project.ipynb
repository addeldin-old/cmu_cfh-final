{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pathlib\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(\"[\\wâ€™'']+\")\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "#this should change based on the folder names for each corpus held in the \"data\" folder\n",
    "corpora = [\"lesbian-fanfic\",\"lesbian-pulp\"]\n",
    "\n",
    "#populate with lists of words that are associated with each other\n",
    "associated_wordlists = {\n",
    "    \"body\": [\"body\", \"breasts\", \"hips\", \"lipstick\", \"hair\", \"brow\", \"arms\", \"legs\", \"lips\",\"waist\"],\n",
    "    \"intimacy\": [\"intimacy\",\"tenderness\", \"tender\", \"loving\", \"warm\", \"warmth\", \"ache\", \"touch\",\"touching\",\"love\"],\n",
    "    \"identity\": [\"lesbian\",\"homosexual\",\"gay\",\"queer\",\"bisexual\",\"self\",\"herself\"]\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are \"utility functions,\" which is to say they perform a task in service\n",
    "of some other core analytic function, e.g. removing punctuatons, getting function words, or reformatting\n",
    "results like ngrams.\n",
    "\"\"\"\n",
    "def tokenizers(text):\n",
    "    \"\"\"\n",
    "    Receives a string and returns a list of the word tokens and sentence tokens, the\n",
    "    sentence tokens being for the purpose of ngram analysis.\n",
    "    \"\"\"\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    return word_tokens, sent_tokens\n",
    "\n",
    "def get_function_words():\n",
    "    \"\"\"\n",
    "    When run, returns a list of function words that is based on a txt file stored in the \"data\" folder.\n",
    "    \"\"\"\n",
    "    filepath = pathlib.Path(\"data/function_words.txt\")\n",
    "    fin = open(filepath)\n",
    "    result = list()                            \n",
    "    for line in fin:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Receives a string and strips any punctuation that is part of string.punctuation, returning\n",
    "    the punctuationless string.\n",
    "    \"\"\"\n",
    "    text_nopunct = text.translate(text.maketrans('', '', punctuation))\n",
    "    return text_nopunct\n",
    "\n",
    "def filter_stops(w):\n",
    "    \"\"\"Receives a string and filters out the word if it is less than three characters or a stop word.\n",
    "    Returns the word if it passes the criteria and nothing if the word does not.\n",
    "    This is to be used as a filter for the collocations NLTK tool.\"\"\"\n",
    "    if len(w) < 3 or w in english_stops:\n",
    "        return w\n",
    "    return None\n",
    "\n",
    "def chapter_segmenter(text):\n",
    "    \"\"\"\n",
    "    Receives read txt file (or, I guess, any string) and returns a list of strings.\n",
    "    The list's items will each be a chapter in the book assuming they are marked as \"chapter X\\n\"\n",
    "    such that X is any mix of numbers, letters, periods, or colons. If there are not chapter\n",
    "    markers, it will simply break the text into 15 roughly equal segments.\n",
    "    \"\"\"\n",
    "    chapters = list()\n",
    "    chapter_pattern = \"chapter [a-z0-9:\\.]+(?=\\n)\"\n",
    "    chapter_headers = re.findall(chapter_pattern,text)\n",
    "    \n",
    "    # checks to ensure there are at least 2 chapter headers, to avoid false positives\n",
    "    if len(chapter_headers) > 2:\n",
    "        \n",
    "        # goes through each chapter header and grabs the text between it and the next chapter header,\n",
    "        # or (if it's the last chapter) grabs all the text from the header to the end\n",
    "        for chapter_index in range(len(chapter_headers)):\n",
    "            if chapter_index < len(chapter_headers) - 1:\n",
    "                chapter_text_pattern = \"(?<={}\\n)[\\s\\S]+(?={}\\n)\".format(chapter_headers[chapter_index],\n",
    "                                                                         chapter_headers[chapter_index+1])\n",
    "                chapter_text = re.search(chapter_text_pattern,text)\n",
    "                chapters.append(chapter_text.group())\n",
    "            else:\n",
    "                chapter_text_pattern = \"{}[\\s\\S]+\".format(chapter_headers[chapter_index])\n",
    "                chapter_text = re.search(chapter_text_pattern,text)\n",
    "                chapters.append(chapter_text.group())\n",
    "    \n",
    "    # if there are no chapter headers, the text is split into 15 roughly equal parts;\n",
    "    # there is a check for the end of each window to make sure it doesn't split a word or sentence;\n",
    "    # instead, it finds the nearest newline (to the left in the line) and breaks the text there\n",
    "    else:\n",
    "        chapters = list()\n",
    "        window = int(len(text)/15)\n",
    "        while len(chapters) < 15:\n",
    "            if len(chapters) < 14:\n",
    "                ind = window * len(chapters)\n",
    "                next_window = ind + window\n",
    "                newline_ind = text.rfind(\"\\n\",ind,next_window)\n",
    "                chapter = text[ind:next_window]\n",
    "                chapters.append(chapter)\n",
    "                ind = newline_ind+1\n",
    "            else:\n",
    "                chapters.append(text[ind:])\n",
    "\n",
    "    return chapters\n",
    "\n",
    "def freq_result_rewriter(raw_result,column_head):\n",
    "    \"\"\"\n",
    "    In service of writing out results to a csv without extra commas breaking up the results,\n",
    "    this function receives a list of tuples containing the frequency and word(s) for some kind of\n",
    "    \"top x words/ngrams\" result, and returns a string of the words or ngrams separated by spaces.\n",
    "    If the result is a list of ngram counts, then it puts the words in the ngram within single quotes\n",
    "    to avoid a lack of clarity of where each ngram starts and ends.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    \n",
    "    # the least indented if/else determines if it's a \"normal\" result or if the result is itself\n",
    "    # a list of results, which would then need to converted from frequency-phrase tuples to a string of\n",
    "    # the phrases\n",
    "    if re.search(\"^top [0-9]+\",column_head) != None:\n",
    "        \n",
    "        # this if/else checks if it's an ngram, because then the phrase is multiple words and needs\n",
    "        # to be converted to a string of those words separated by spaces and surrounded by single quotes\n",
    "        if re.search(\"grams$\",column_head) != None or re.search(\"collocations$\",column_head) != None:\n",
    "            \n",
    "            # this for loop goes through the ngram results, grabs the ngram tuple, \n",
    "            # and converts it into a string surrounded by single quotes, then adds them to a larger\n",
    "            # result string\n",
    "            for ngram_result_index in range(len(raw_result)):\n",
    "                gram_tuple = raw_result[ngram_result_index][1]\n",
    "                gram_list = list(gram_tuple)\n",
    "                gram = \" \".join(gram_list)\n",
    "                gram = \"'{}'\".format(gram)\n",
    "                \n",
    "                # this if/else ensures there is not an extra space after the last word\n",
    "                if ngram_result_index < len(raw_result)-1:\n",
    "                    result += gram+\" \"\n",
    "                else:\n",
    "                    result += gram\n",
    "        else:\n",
    "            \n",
    "            # this for loop goes through a non-ngram frequency-phrase result and fetches the phrase,\n",
    "            # adding it to a larger string of the results separated by spaces\n",
    "            for word_result_index in range(len(raw_result)):\n",
    "                word = raw_result[word_result_index][1]\n",
    "                if word_result_index < len(raw_result)-1:\n",
    "                        result += word+\" \"\n",
    "                else:\n",
    "                    result += word   \n",
    "                    \n",
    "    else:\n",
    "        result = str(raw_result)\n",
    "    return result\n",
    "\n",
    "def get_collocations(words,stops=None):\n",
    "    \"\"\"\n",
    "    Uses NLTK's bigram collocation finder to receive a list of words (and whether or not to filter stop words)\n",
    "    and find all bigram collocations that appear at least three times.\n",
    "    Returns a list of tuples that each contain two items: a tuple of the two collocated words, and a number\n",
    "    representing the number of appearances.\n",
    "    \"\"\"    \n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "    if stops == False:\n",
    "        bcf.apply_word_filter(filter_stops)\n",
    "    elif stops == True:\n",
    "        pass\n",
    "    bcf.apply_freq_filter(2)  \n",
    "    result = list(bcf.ngram_fd.items())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are file-oriented, which is to say they are involved in converting\n",
    "strings to file objects (e.g. a folder to a list of filepaths), or in writing out results to a\n",
    "csv file.\n",
    "\"\"\"\n",
    "\n",
    "def folder_to_filepaths(folder):\n",
    "    \"\"\"\n",
    "    Receives the name of a folder within a subfolder called \"data\" and returns a list of\n",
    "    Path objects, one for each file within the folder.\n",
    "    \"\"\"\n",
    "    folder_path = pathlib.Path(\"data/{}\".format(folder))\n",
    "    files = folder_path.glob('*.txt')\n",
    "    file_names = list()\n",
    "    for file in files:\n",
    "        file_names.append(file)\n",
    "    return file_names\n",
    "\n",
    "def file_reader(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file and returns a string of the text in that file.\n",
    "    \"\"\"\n",
    "    with open(filepath,encoding=\"utf-8\") as fin:\n",
    "        f_text = fin.read().strip().lower()\n",
    "    return f_text\n",
    "\n",
    "def csv_creator(folder_name,header_items):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a list of the column names,\n",
    "    and then creates a file for that corpus with the header row written in.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"w\") as csv_out:\n",
    "        counter = 0\n",
    "        for header_item in header_items:\n",
    "            if counter < len(header_items)-1:\n",
    "                csv_out.write(header_item+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(header_item+\"\\n\")\n",
    "                \n",
    "def csv_creator_sentiments(folder_name,header_items):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a list of the column names,\n",
    "    and then creates a file for that corpus with the header row written in. Note that\n",
    "    the name of the file is different than the regular csv_creator.\n",
    "    \"\"\"\n",
    "    csv_name = \"sentiments_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"w\") as csv_out:\n",
    "        counter = 0\n",
    "        for header_item in header_items:\n",
    "            if counter < len(header_items)-1:\n",
    "                csv_out.write(header_item+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(header_item+\"\\n\")\n",
    "\n",
    "def csv_writer(folder_name,results):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a dictionary of results for \n",
    "    a document or corpus, and then writes a new row into the respective corpus results csv \n",
    "    with that document's or corpus's results. Note that checks to see if the result is a \n",
    "    \"top\" number of words (e.g. most frequent words, most frequent ngrams), and calls\n",
    "    the function that reformats the result if so to make sure there are not extra \n",
    "    commas while writing out the result.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"a\") as csv_out:\n",
    "        counter = 0\n",
    "        \n",
    "        # this for loop goes through the dictionary of results, fetches the value (so an individual result),\n",
    "        # and then formats the result\n",
    "        for key in results:\n",
    "            unformatted_result = results[key]\n",
    "            result = freq_result_rewriter(unformatted_result,key)\n",
    "            \n",
    "            # this if/else ensures that the line ends with a newline, not a comma\n",
    "            if counter < len(results)-1:\n",
    "                csv_out.write(result+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(result+\"\\n\")\n",
    "                \n",
    "def csv_writer_sentiments(folder_name,results):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a dictionary of results for \n",
    "    a document, with the dictionary mapping a filename to a dictionary that maps chapter\n",
    "    numbers to sentiment scores. Each row is written out with the filename for a text,\n",
    "    then the score for each chapter.\n",
    "    \"\"\"\n",
    "    csv_name = \"sentiments_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    \n",
    "    # this for loop finds the text with the most chapters, and makes sure the header for the csv\n",
    "    # goes up to the highest chapter that will appear\n",
    "    chapters_max = None\n",
    "    for text in results:\n",
    "        if chapters_max == None:\n",
    "            chapters_max = len(results[text].items())\n",
    "        else:\n",
    "            if len(results[text]) > chapters_max:\n",
    "                chapters_max = len(results[text].items())                \n",
    "    chapters_header = [\"filename\"]\n",
    "    chapters_header.extend([str(i+1) for i in range(chapters_max)])\n",
    "    csv_creator_sentiments(folder_name,chapters_header)\n",
    "    \n",
    "    with open(csv_path,\"a\") as csv_out:\n",
    "        # this for loop goes through the dictionary of results, writes out the filename,\n",
    "        # and then goes through each text's dictionary, writing out each sentiment score\n",
    "        # for each chapter\n",
    "        for text in results:\n",
    "            result = results[text]\n",
    "            csv_out.write(text+\",\")\n",
    "            counter = 0\n",
    "            # this if/else ensures that the line ends with a newline, not a comma\n",
    "            while counter < len(result):\n",
    "                chapter_score = str(result[counter+1])\n",
    "                if counter < len(result)-1:\n",
    "                    csv_out.write(chapter_score+\",\")\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    csv_out.write(chapter_score+\"\\n\")\n",
    "                    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on finding common words or sequences of words.\n",
    "\"\"\"\n",
    "\n",
    "def freq_words(words, num_words):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and the number of \"top words\" to find (let's call it x),\n",
    "    and determines the x most frequently occurring words.\n",
    "    Returns a list of x tuples, with each containing the count and word associated with it.\n",
    "    \"\"\"\n",
    "    function_words = get_function_words()\n",
    "    word_counts = dict()\n",
    "    \n",
    "    # goes through the tokens list, creating a dictionary that relates the\n",
    "    # word to how many times it appears, so long as it is not a function word\n",
    "    for word in words:\n",
    "        if word not in function_words:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "        else:\n",
    "            pass\n",
    "    count_word_pairs = list()\n",
    "    \n",
    "    # converts the dictionary word-count relationships to a tuple with the count first, then the word\n",
    "    for key, value in word_counts.items():\n",
    "        count_word_pairs.append((value,key))\n",
    "    count_word_pairs.sort(reverse=True)\n",
    "    return count_word_pairs[0:num_words]\n",
    "\n",
    "def freq_ngrams(sentences, n, num_ngrams):\n",
    "    \"\"\"\n",
    "    Receives a list of sentence tokens, the n ngrams to find, and the number of \"top ngrams\" to find (x),\n",
    "    and determines the x most frequently occuring ngrams, without traversing sentences.\n",
    "    Returns a list of x tuples, with each containing the count and \n",
    "    a tuple of the words in the ngram associated with it.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    \n",
    "    # goes through each sentence, tokenizes it, and then gathers every ngram for the sentence of size n\n",
    "    for sent in sentences:\n",
    "        words = tokenizer.tokenize(sent)\n",
    "        ngram_obj = nltk.ngrams(words, n)\n",
    "        sent_ngrams = list(ngram_obj)\n",
    "        ngrams.extend(sent_ngrams)\n",
    "    ngram_counts = dict()\n",
    "    \n",
    "    # goes through each ngram and creates a dictionary relating the ngram to how many times it appears\n",
    "    for ngram in ngrams:\n",
    "        if ngram not in ngram_counts:\n",
    "            ngram_counts[ngram] = 1\n",
    "        else:\n",
    "            ngram_counts[ngram] += 1\n",
    "    count_ngram_pairs = list()\n",
    "    for key,value in ngram_counts.items():\n",
    "        count_ngram_pairs.append((value,key))\n",
    "    count_ngram_pairs.sort(reverse=True)\n",
    "    \n",
    "    # this if/else ensures there won't be an error if the number of ngrams is less than the requested number\n",
    "    if len(count_ngram_pairs) < num_ngrams:\n",
    "        return count_ngram_pairs\n",
    "    else:\n",
    "        return count_ngram_pairs[0:num_ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses around pronouns.\n",
    "\"\"\"\n",
    "\n",
    "def top_pronoun_verb_pairs(words, num_words, pronoun):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and determines the top x number of verbs \n",
    "    (in lemmatized form) most frequently associated with a given pronoun.\n",
    "    Returns a list of x tuples, each containing the count and word associated with the pronoun.\n",
    "    \"\"\"\n",
    "    #pos_tokens = nltk.pos_tag(words)\n",
    "    associated_verbs = list()\n",
    "    verb_counts = dict()\n",
    "    pronouns_plus2 = list()\n",
    "    \n",
    "    # loops through each word, and if the word is the requested pronoun, it adds that word\n",
    "    # plus the next two as a list to a larger list of all instances (so, the result is)\n",
    "    # a list of lists, with each sublist being a set of three words: the pronoun, then the next two words\n",
    "    # in the original text\n",
    "    for index in range(len(words)):\n",
    "        if words[index] == pronoun:\n",
    "            pronouns_plus2.append(words[index:index+3])\n",
    "            \n",
    "    # loops through the list, one set of three words at a time, populating \n",
    "    # a list of the verbs following the pronoun\n",
    "    for pronoun_plus2 in pronouns_plus2:\n",
    "        pos_tagged = nltk.pos_tag(pronoun_plus2)\n",
    "        \n",
    "        # if the word directly after the pronoun is a present/past tense verb, it adds\n",
    "        # the verb in its base form to the list of verbs\n",
    "        if pos_tagged[1][1] in [\"VB\", \"VBD\", \"VBZ\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[1][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "        # if the second word after the pronoun is a participle or gerund, it adds it\n",
    "        # to make sure things like \"she is walking\" have \"walk\" added to the list of verbs\n",
    "        if pos_tagged[2][1] in [\"VBG\", \"VBN\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[2][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "    # loops through the associated verbs and creates a dictionary counting the number of occurrences\n",
    "    for verb in associated_verbs:\n",
    "        if verb not in verb_counts:\n",
    "            verb_counts[verb] = 1\n",
    "        else:\n",
    "            verb_counts[verb] += 1\n",
    "    verb_count_list = list()\n",
    "    for verb,count in verb_counts.items():\n",
    "        verb_count_list.append((count,verb))\n",
    "    verb_count_list.sort(reverse=True)\n",
    "    return verb_count_list[0:num_words]\n",
    "\n",
    "def pronoun_subj_ratio(words, pronoun_subj, pronoun_obj):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens, the subj version of a pronoun (e.g. \"she\"), \n",
    "    and a list of the object versions (e.g. [\"her\", \"hers\"]).\n",
    "    I chose this format because it does not predetermine the available pronouns.\n",
    "    Returns a ratio in decimal form with the subject pronoun as the numerator and \n",
    "    all pronoun counts as the denominator. So, the higher the ratio, the higher frequency of subject pronoun.\n",
    "    \"\"\"\n",
    "    subj_count = 0\n",
    "    obj_count = 0\n",
    "    for word in words:\n",
    "        if word == pronoun_subj:\n",
    "            subj_count += 1\n",
    "        elif word in pronoun_obj:\n",
    "            obj_count += 1\n",
    "    subj_ratio = subj_count / (obj_count + subj_count)\n",
    "    return subj_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses involving collocations and sentiment.\n",
    "\"\"\"\n",
    "\n",
    "def get_top_collocations(coll_list,num_colls):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations and the number of top results to fetch.\n",
    "    Returns the top num_colls collocations.\n",
    "    \"\"\"\n",
    "    rev_colls = list()\n",
    "    \n",
    "    #rewrite results so they can be sorted by count)\n",
    "    for coll, count in coll_list:\n",
    "        rev_colls.append((count, coll))\n",
    "    rev_colls.sort(reverse=True)\n",
    "    \n",
    "    return rev_colls[0:num_colls]\n",
    "\n",
    "def coll_associations(coll_list, associated_terms, num_results):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations, a list of terms to find associated words for, \n",
    "    and a number of desired top results.\n",
    "    Returns a list of the top num_results words that are collocated with the seeded terms, in lemma form.\n",
    "    \"\"\"\n",
    "    collocated_term_counts = dict()\n",
    "    \n",
    "    associated_lemmas = list()\n",
    "    #turn the list of terms to find collocations for into lemmatized forms\n",
    "    for term in associated_terms:\n",
    "        lemma = lemmatizer.lemmatize(term)\n",
    "        associated_lemmas.append(lemma)\n",
    "\n",
    "    \n",
    "    #loop through the items in the list of collocations to see if either word (in lemma form)\n",
    "    #is in the list of lemmas for which associated collocations are being sought; \n",
    "    #if so, it adds the associated collocation (not the seed term) to a dictionary, and adds the count\n",
    "    for terms, count in coll_list:\n",
    "        term_lemmas = (lemmatizer.lemmatize(terms[0]), lemmatizer.lemmatize(terms[1]))\n",
    "        if term_lemmas[0] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[1]\n",
    "        elif term_lemmas[1] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[0]\n",
    "        else:\n",
    "            associated_lemma = None\n",
    "        #checks to see if associated lemma is in the dictionary already; if not, it sets the count\n",
    "        #to the collocation count; if so, it adds the count to the existing count\n",
    "        if associated_lemma != None and associated_lemma not in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] = count\n",
    "        elif associated_lemma != None and associated_lemma in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] += count\n",
    "            \n",
    "            \n",
    "    count_coll_pairs = list()\n",
    "    for key,value in collocated_term_counts.items():\n",
    "        count_coll_pairs.append((value,key))\n",
    "    count_coll_pairs.sort(reverse=True) \n",
    "    \n",
    "    return count_coll_pairs[0:num_results]\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    Receives a read txt file (or any string) and returns two items:\n",
    "    1) a sentiment polarity for the entire text;\n",
    "    2) a dictionary mapping a chapter number to that chapter's sentiment polarity.\n",
    "    \"\"\"\n",
    "    chapter_scores = dict()\n",
    "    chapters = chapter_segmenter(text)\n",
    "    chapter_number = 1\n",
    "    \n",
    "    # loops through each chapter, creates a TextBlob object, and determines the polarity of the chapter;\n",
    "    # then, it adds the polarity to a dictionary mapping chapter number to sentiment polarity\n",
    "    for chapter in chapters:\n",
    "        tbc = TextBlob(chapter)\n",
    "        chapter_sentiment_polarity = tbc.sentiment.polarity\n",
    "        chapter_scores[chapter_number] = chapter_sentiment_polarity\n",
    "        chapter_number += 1\n",
    "    tbt = TextBlob(text)\n",
    "    text_polarity = tbt.sentiment.polarity\n",
    "    return text_polarity, chapter_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block collate the file-level and corpus-level analytic functions,\n",
    "as well as the file input-output functions, taking the code from start (folder names representing\n",
    "corpora) to finish (writing out the results to csvs). In a sense, they provide the infrastructure\n",
    "by which the more tool- and task-oriented functions come together to perform a set of analyses\n",
    "and share the results with the user.\n",
    "\"\"\"\n",
    "\n",
    "def file_analysis(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file, tokenizes it, and returns:\n",
    "    1) a dictionary of the results, associating the name of a measure or result with the result,\n",
    "    acquired by running the relevant analytic function.\n",
    "    2) a tuple of the word and sentence tokens for use in the corpus-level analysis.\n",
    "    \"\"\"\n",
    "    doc_results = dict()\n",
    "    text = file_reader(filepath)\n",
    "    word_tokens, sentence_tokens = tokenizers(text)\n",
    "    collocations = get_collocations(word_tokens, stops = False)\n",
    "    text_sentiment, chapter_sentiments = sentiment_analysis(text)\n",
    "    doc_results[\"filename\"] = filepath.name\n",
    "    doc_results[\"top 10 words\"] = freq_words(word_tokens,10)\n",
    "    doc_results[\"top 10 bigrams\"] = freq_ngrams(sentence_tokens, 2, 10)\n",
    "    doc_results[\"top 10 verbs_'she'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"she\")\n",
    "    doc_results[\"top 10 verbs_'he'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"he\")\n",
    "    doc_results[\"top 10 verbs_'they'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"they\")\n",
    "    doc_results[\"pronoun ratio_'she'\"] = pronoun_subj_ratio(word_tokens, \"she\", [\"her\",\"hers\"])\n",
    "    doc_results[\"pronoun ratio_'he'\"] = pronoun_subj_ratio(word_tokens, \"he\", [\"his\",\"him\"])\n",
    "    doc_results[\"pronoun ratio_'they'\"] = pronoun_subj_ratio(word_tokens, \"they\", [\"their\",\"theirs\"])  \n",
    "    doc_results[\"top 10 collocations\"] = get_top_collocations(collocations, 10)\n",
    "    doc_results[\"sentiment polarity\"] = text_sentiment\n",
    "    \n",
    "    #add collocated related terms results by looping through lists of associated terms\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        doc_results[coll_header] = coll_associations(collocations, associated_wordlists[wordlist_key], 10)\n",
    "\n",
    "    \n",
    "    # this passes the tokens as a result so it can be used in analyzing the corpus as a whole, as\n",
    "    # well as the collocations and dictionary of results mapping chapter numbers to sentiment scores\n",
    "    tokens = (word_tokens, sentence_tokens)\n",
    "    return doc_results, tokens, collocations, chapter_sentiments\n",
    "\n",
    "def process_files(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, acquires a list of all files within it, \n",
    "    analyzes each using file_analysis(), and generates:\n",
    "    1) a list containing the dictionaries full of results for each documents\n",
    "    2) a tuple containing the combined word tokens and combined sentence tokens for the corpus\n",
    "    \"\"\"\n",
    "    file_objs = folder_to_filepaths(folder)\n",
    "    files_results = list()\n",
    "    corpus_word_tokens = list()\n",
    "    corpus_sentence_tokens = list()\n",
    "    corpus_collocations = list()\n",
    "    corpus_sentiments = dict()\n",
    "    \n",
    "    # goes through the file path objects in a folder, does file_analysis, adds the dictionary of results\n",
    "    # to a list, and creates lists of all tokens (word and sentence) in the corpus\n",
    "    for file_obj in file_objs:\n",
    "        file_result, file_tokens, file_colls, file_sents = file_analysis(file_obj)\n",
    "        files_results.append(file_result)\n",
    "        corpus_word_tokens.extend(file_tokens[0])\n",
    "        corpus_sentence_tokens.extend(file_tokens[1])\n",
    "        corpus_collocations.extend(file_colls)\n",
    "        \n",
    "        # this line builds the dictionary that is passed to csv_writer_sentiments, with the name\n",
    "        # of the file being mapped to the dictionary of results mapping chapter num to sent. polarity\n",
    "        corpus_sentiments[file_obj.stem] = file_sents\n",
    "    corpus_tokens = (corpus_word_tokens,corpus_sentence_tokens)\n",
    "    \n",
    "    # passes the list of dictionaries and the tuple of corpus tokens for corpus-level analysis, as well as\n",
    "    # the collocations for the entire corpus and dictionary of filenames and sentiment scores\n",
    "    return files_results, corpus_tokens, corpus_collocations, corpus_sentiments\n",
    "\n",
    "def process_folder(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, retrieves the list of results/corpus-level tokens for it\n",
    "    using process_files(), retrieves the results at the corpus level, and appends the corpus-level\n",
    "    results to the list of results, returning this \"complete\" list of results for the corpus and\n",
    "    documents within it.\n",
    "    \"\"\"\n",
    "    documents_results, documents_tokens, documents_collocations, documents_sentiments = process_files(folder)\n",
    "    print(\"file-level analysis done for corpus: {}\".format(folder))\n",
    "    folder_top10_freq = freq_words(documents_tokens[0], 10)\n",
    "    folder_top10_bigrams = freq_ngrams(documents_tokens[1], 2, 10)\n",
    "    folder_top10_verbs_she = top_pronoun_verb_pairs(documents_tokens[0], 10, \"she\")\n",
    "    folder_top10_verbs_he = top_pronoun_verb_pairs(documents_tokens[0], 10, \"he\")\n",
    "    folder_top10_verbs_they = top_pronoun_verb_pairs(documents_tokens[0], 10, \"they\")\n",
    "    folder_pronoun_ratio_she = pronoun_subj_ratio(documents_tokens[0], \"she\", [\"her\",\"hers\"])\n",
    "    folder_pronoun_ratio_he = pronoun_subj_ratio(documents_tokens[0], \"he\", [\"his\",\"him\"])\n",
    "    folder_pronoun_ratio_they = pronoun_subj_ratio(documents_tokens[0], \"they\", [\"their\",\"theirs\"])\n",
    "    folder_top10_collocations = get_top_collocations(documents_collocations, 10)\n",
    "\n",
    "    \n",
    "    # I realize I could've done this in the above lines, but I wanted to make it clear how\n",
    "    # the corpus analysis uses mostly different or modified functions for the results and adds that\n",
    "    folder_result = {\"filename\": folder, \"top 10 words\": folder_top10_freq, \n",
    "                     \"top 10 bigrams\": folder_top10_bigrams, \n",
    "                     \"top 10 verbs_'she'\": folder_top10_verbs_she, \n",
    "                     \"top 10 verbs_'he'\": folder_top10_verbs_he,\n",
    "                     \"top 10 verbs_'they'\": folder_top10_verbs_they,\n",
    "                     \"pronoun ratio_'she'\": folder_pronoun_ratio_she, \n",
    "                     \"pronoun ratio_'he'\": folder_pronoun_ratio_he,\n",
    "                     \"pronoun ratio_'they'\": folder_pronoun_ratio_they,\n",
    "                     \"top 10 collocations\": folder_top10_collocations,\n",
    "                     #the polarity of the corpus doesn't seem useful, but I don't want to leave the row blank\n",
    "                     \"sentiment polarity\": \"na\"}\n",
    "\n",
    "    coll_associations_results = dict()\n",
    "    #add collocated related terms results by looping through lists of associated terms, creating dictionary\n",
    "    #so that the specific name can be written out in the header row for the csv\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        folder_result[coll_header] = coll_associations(documents_collocations, \n",
    "                                                                    associated_wordlists[wordlist_key], \n",
    "                                                                    10)\n",
    "    \n",
    "    print(\"corpus-level analysis done for corpus: {}\".format(folder))\n",
    "    documents_results.append(folder_result)\n",
    "    \n",
    "    #passes documents_sentiments forward to be written out in the run() function\n",
    "    return documents_results, documents_sentiments\n",
    "\n",
    "def write_results(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name, runs process_folder() on it (which in turn performs the analyses per \n",
    "    document and corpus, and collates the results into a list of dictionaries), creates the\n",
    "    results csv for the folder with a header row, and then writes each dictionary of results\n",
    "    as a row.\n",
    "    \"\"\"\n",
    "    corpus_results, corpus_sentiments = process_folder(folder)\n",
    "    header = list(corpus_results[0].keys())\n",
    "    csv_creator(folder,header)\n",
    "    \n",
    "    # note that this for loop goes through the list of dictionaries of results and, for each one,\n",
    "    # writes them out to the csv, line by line\n",
    "    for document_result in corpus_results:\n",
    "        csv_writer(folder, document_result)\n",
    "        \n",
    "    csv_writer_sentiments(folder,corpus_sentiments)\n",
    "    \n",
    "    print(\"results written out to csvs for corpus: {}\\n--------------------\".format(folder))\n",
    "    #added this so I can do some analytics as seen at the end of this notebook\n",
    "    return corpus_results\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Iterates through the corpora as defined at the top, which should reflect the folder names in the\n",
    "    \"data\" folder, and runs write_results() on it (which in turn processes the files, acquires the\n",
    "    results, and writes the results out to a csv file).\n",
    "    \"\"\"\n",
    "    global res_dict\n",
    "    res_dict = dict()\n",
    "    for corpus in corpora:\n",
    "        res = write_results(corpus)\n",
    "        res_dict[corpus] = res\n",
    "    print(\"done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-level analysis done for corpus: lesbian-fanfic\n",
      "corpus-level analysis done for corpus: lesbian-fanfic\n",
      "results written out to csvs for corpus: lesbian-fanfic\n",
      "--------------------\n",
      "file-level analysis done for corpus: lesbian-pulp\n",
      "corpus-level analysis done for corpus: lesbian-pulp\n",
      "results written out to csvs for corpus: lesbian-pulp\n",
      "--------------------\n",
      "done!!\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block run some basic analytics on results to help me\n",
    "find inroads for my analysis. It's mostly just to help me and I wouldn't include it were I to share\n",
    "this code, but I am temporarily including it here.\n",
    "\"\"\"\n",
    "commonalities = dict()\n",
    "\n",
    "for text in res_dict[\"lesbian-fanfic\"]:\n",
    "    for result in text:\n",
    "        commonalities[result] = None\n",
    "        \n",
    "\n",
    "\n",
    "def read_csv(corpus_name):\n",
    "    filename = \"results_{}.csv\".format(corpus_name)\n",
    "    filepath = pathlib.Path(filename)\n",
    "    corpus_rows = list()\n",
    "    with open(filepath,\"r\") as file_obj:\n",
    "        content = file_obj.read()\n",
    "    rows = content.split(\"\\n\")\n",
    "    corpus_rows.extend(rows[0:-1])\n",
    "    return corpus_rows\n",
    "\n",
    "\n",
    "\n",
    "for corpus in corpora:\n",
    "    csv_dict = dict()\n",
    "    rows = read_csv(corpus)\n",
    "    csv_dict[\"corpus\"] = corpus\n",
    "    column_heads = rows[0].split(\",\")\n",
    "    relevant_headers = column_heads[1:2]+column_heads[3:6]+column_heads[9:]\n",
    "    for column_head in relevant_headers:\n",
    "        csv_dict[column_head] = None\n",
    "    for row in rows[1:]:\n",
    "        split_row = row.split(\",\")\n",
    "        split_row = split_row[1:6]+split_row[9:]\n",
    "        split_row.remove(split_row[1])\n",
    "        for column_ind in range(len(split_row)):\n",
    "            result = []\n",
    "            for word in split_row[column_ind].split():\n",
    "                result.append(word)\n",
    "            if csv_dict[relevant_headers[column_ind]] == None:\n",
    "                csv_dict[relevant_headers[column_ind]] = result\n",
    "            else:\n",
    "                csv_dict[relevant_headers[column_ind]].extend(result)\n",
    "    print(corpus+\"\\n\")\n",
    "    for relevant_header in relevant_headers:\n",
    "        if relevant_header != \"top 10 collocations\":\n",
    "            print(relevant_header)\n",
    "            counts = {}\n",
    "            counts_bynum = []\n",
    "            res_list = csv_dict[relevant_header]\n",
    "            for word in res_list:\n",
    "                if word not in counts:\n",
    "                    counts[word] = 1\n",
    "                else:\n",
    "                    counts[word] += 1\n",
    "            for key in counts:\n",
    "                if counts[key] >= 3:\n",
    "                    counts_bynum.append((counts[key],key))\n",
    "            counts_bynum.sort(reverse=True)\n",
    "            print(counts_bynum)\n",
    "            print(\"----------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
