{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pathlib\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(\"[\\wâ€™'']+\")\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "#this should change based on the folder names for each corpus held in the \"data\" folder\n",
    "corpora = [\"lesbian-fanfic\",\"lesbian-pulp\"]\n",
    "\n",
    "#populate with lists of words that are associated with each other\n",
    "associated_wordlists = {\n",
    "    \"body\": [\"body\", \"breasts\", \"hips\", \"lipstick\", \"hair\", \"brow\", \"arms\", \"legs\", \"lips\",\"waist\"],\n",
    "    \"intimacy\": [\"intimacy\",\"tenderness\", \"tender\", \"loving\", \"warm\", \"warmth\", \"ache\", \"touch\",\"touching\",\"love\"],\n",
    "    \"identity\": [\"lesbian\",\"homosexual\",\"gay\",\"queer\",\"bisexual\",\"self\",\"herself\"]\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are \"utility functions,\" which is to say they perform a task in service\n",
    "of some other core analytic function, e.g. removing punctuatons, getting function words, or reformatting\n",
    "results like ngrams.\n",
    "\"\"\"\n",
    "def tokenizers(text):\n",
    "    \"\"\"\n",
    "    Receives a string and returns a list of the word tokens and sentence tokens, the\n",
    "    sentence tokens being for the purpose of ngram analysis.\n",
    "    \"\"\"\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    return word_tokens, sent_tokens\n",
    "\n",
    "def get_function_words():\n",
    "    \"\"\"\n",
    "    When run, returns a list of function words that is based on a txt file stored in the \"data\" folder.\n",
    "    \"\"\"\n",
    "    filepath = pathlib.Path(\"data/function_words.txt\")\n",
    "    fin = open(filepath)\n",
    "    result = list()                            \n",
    "    for line in fin:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Receives a string and strips any punctuation that is part of string.punctuation, returning\n",
    "    the punctuationless string.\n",
    "    \"\"\"\n",
    "    text_nopunct = text.translate(text.maketrans('', '', punctuation))\n",
    "    return text_nopunct\n",
    "\n",
    "def filter_stops(w):\n",
    "    \"\"\"Receives a string and filters out the word if it is less than three characters or a stop word.\n",
    "    Returns the word if it passes the criteria and nothing if the word does not.\n",
    "    This is to be used as a filter for the collocations NLTK tool.\"\"\"\n",
    "    if len(w) < 3 or w in english_stops:\n",
    "        return w\n",
    "    return None\n",
    "\n",
    "def freq_result_rewriter(raw_result,column_head):\n",
    "    \"\"\"\n",
    "    In service of writing out results to a csv without extra commas breaking up the results,\n",
    "    this function receives a list of tuples containing the frequency and word(s) for some kind of\n",
    "    \"top x words/ngrams\" result, and returns a string of the words or ngrams separated by spaces.\n",
    "    If the result is a list of ngram counts, then it puts the words in the ngram within single quotes\n",
    "    to avoid a lack of clarity of where each ngram starts and ends.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    \n",
    "    # the least indented if/else determines if it's a \"normal\" result or if the result is itself\n",
    "    # a list of results, which would then need to converted from frequency-phrase tuples to a string of\n",
    "    # the phrases\n",
    "    if re.search(\"^top [0-9]+\",column_head) != None:\n",
    "        \n",
    "        # this if/else checks if it's an ngram, because then the phrase is multiple words and needs\n",
    "        # to be converted to a string of those words separated by spaces and surrounded by single quotes\n",
    "        if re.search(\"grams$\",column_head) != None or re.search(\"collocations$\",column_head) != None:\n",
    "            \n",
    "            # this for loop goes through the ngram results, grabs the ngram tuple, \n",
    "            # and converts it into a string surrounded by single quotes, then adds them to a larger\n",
    "            # result string\n",
    "            for ngram_result_index in range(len(raw_result)):\n",
    "                gram_tuple = raw_result[ngram_result_index][1]\n",
    "                gram_list = list(gram_tuple)\n",
    "                gram = \" \".join(gram_list)\n",
    "                gram = \"'{}'\".format(gram)\n",
    "                \n",
    "                # this if/else ensures there is not an extra space after the last word\n",
    "                if ngram_result_index < len(raw_result)-1:\n",
    "                    result += gram+\" \"\n",
    "                else:\n",
    "                    result += gram\n",
    "        else:\n",
    "            \n",
    "            # this for loop goes through a non-ngram frequency-phrase result and fetches the phrase,\n",
    "            # adding it to a larger string of the results separated by spaces\n",
    "            for word_result_index in range(len(raw_result)):\n",
    "                word = raw_result[word_result_index][1]\n",
    "                if word_result_index < len(raw_result)-1:\n",
    "                        result += word+\" \"\n",
    "                else:\n",
    "                    result += word   \n",
    "                    \n",
    "    else:\n",
    "        result = str(raw_result)\n",
    "    return result\n",
    "\n",
    "def get_collocations(words,stops=None):\n",
    "    \"\"\"\n",
    "    Uses NLTK's bigram collocation finder to receive a list of words (and whether or not to filter stop words)\n",
    "    and find all bigram collocations that appear at least three times.\n",
    "    Returns a list of tuples that each contain two items: a tuple of the two collocated words, and a number\n",
    "    representing the number of appearances.\n",
    "    \"\"\"    \n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "    if stops == False:\n",
    "        bcf.apply_word_filter(filter_stops)\n",
    "    elif stops == True:\n",
    "        pass\n",
    "    bcf.apply_freq_filter(2)  \n",
    "    result = list(bcf.ngram_fd.items())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are file-oriented, which is to say they are involved in converting\n",
    "strings to file objects (e.g. a folder to a list of filepaths), or in writing out results to a\n",
    "csv file.\n",
    "\"\"\"\n",
    "\n",
    "def folder_to_filepaths(folder):\n",
    "    \"\"\"\n",
    "    Receives the name of a folder within a subfolder called \"data\" and returns a list of\n",
    "    Path objects, one for each file within the folder.\n",
    "    \"\"\"\n",
    "    folder_path = pathlib.Path(\"data/{}\".format(folder))\n",
    "    files = folder_path.glob('*.txt')\n",
    "    file_names = list()\n",
    "    for file in files:\n",
    "        file_names.append(file)\n",
    "    return file_names\n",
    "\n",
    "def file_reader(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file and returns a string of the text in that file.\n",
    "    \"\"\"\n",
    "    with open(filepath,encoding=\"utf-8\") as fin:\n",
    "        f_text = fin.read().strip().lower()\n",
    "    return f_text\n",
    "\n",
    "def csv_creator(folder_name,header_items):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a list of the column names,\n",
    "    and then creates a file for that corpus with the header row written in.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"w\") as csv_out:\n",
    "        counter = 0\n",
    "        for header_item in header_items:\n",
    "            if counter < len(header_items)-1:\n",
    "                csv_out.write(header_item+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(header_item+\"\\n\")\n",
    "\n",
    "def csv_writer(folder_name,results):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a dictionary of results for \n",
    "    a document or corpus, and then writes a new row into the respective corpus results csv \n",
    "    with that document's or corpus's results. Note that checks to see if the result is a \n",
    "    \"top\" number of words (e.g. most frequent words, most frequent ngrams), and calls\n",
    "    the function that reformats the result if so to make sure there are not extra \n",
    "    commas while writing out the result.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"a\") as csv_out:\n",
    "        counter = 0\n",
    "        \n",
    "        # this for loop goes through the dictionary of results, fetches the value (so an individual result),\n",
    "        # and then formats the result\n",
    "        for key in results:\n",
    "            unformatted_result = results[key]\n",
    "            result = freq_result_rewriter(unformatted_result,key)\n",
    "            \n",
    "            # this if/else ensures that the line ends with a newline, not a comma\n",
    "            if counter < len(results)-1:\n",
    "                csv_out.write(result+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(result+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on finding common words or sequences of words.\n",
    "\"\"\"\n",
    "\n",
    "def freq_words(words, num_words):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and the number of \"top words\" to find (let's call it x),\n",
    "    and determines the x most frequently occurring words.\n",
    "    Returns a list of x tuples, with each containing the count and word associated with it.\n",
    "    \"\"\"\n",
    "    function_words = get_function_words()\n",
    "    word_counts = dict()\n",
    "    \n",
    "    # goes through the tokens list, creating a dictionary that relates the\n",
    "    # word to how many times it appears, so long as it is not a function word\n",
    "    for word in words:\n",
    "        if word not in function_words:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "        else:\n",
    "            pass\n",
    "    count_word_pairs = list()\n",
    "    \n",
    "    # converts the dictionary word-count relationships to a tuple with the count first, then the word\n",
    "    for key, value in word_counts.items():\n",
    "        count_word_pairs.append((value,key))\n",
    "    count_word_pairs.sort(reverse=True)\n",
    "    return count_word_pairs[0:num_words]\n",
    "\n",
    "def freq_ngrams(sentences, n, num_ngrams):\n",
    "    \"\"\"\n",
    "    Receives a list of sentence tokens, the n ngrams to find, and the number of \"top ngrams\" to find (x),\n",
    "    and determines the x most frequently occuring ngrams, without traversing sentences.\n",
    "    Returns a list of x tuples, with each containing the count and \n",
    "    a tuple of the words in the ngram associated with it.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    \n",
    "    # goes through each sentence, tokenizes it, and then gathers every ngram for the sentence of size n\n",
    "    for sent in sentences:\n",
    "        words = tokenizer.tokenize(sent)\n",
    "        ngram_obj = nltk.ngrams(words, n)\n",
    "        sent_ngrams = list(ngram_obj)\n",
    "        ngrams.extend(sent_ngrams)\n",
    "    ngram_counts = dict()\n",
    "    \n",
    "    # goes through each ngram and creates a dictionary relating the ngram to how many times it appears\n",
    "    for ngram in ngrams:\n",
    "        if ngram not in ngram_counts:\n",
    "            ngram_counts[ngram] = 1\n",
    "        else:\n",
    "            ngram_counts[ngram] += 1\n",
    "    count_ngram_pairs = list()\n",
    "    for key,value in ngram_counts.items():\n",
    "        count_ngram_pairs.append((value,key))\n",
    "    count_ngram_pairs.sort(reverse=True)\n",
    "    \n",
    "    # this if/else ensures there won't be an error if the number of ngrams is less than the requested number\n",
    "    if len(count_ngram_pairs) < num_ngrams:\n",
    "        return count_ngram_pairs\n",
    "    else:\n",
    "        return count_ngram_pairs[0:num_ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses around pronouns.\n",
    "\"\"\"\n",
    "\n",
    "def top_pronoun_verb_pairs(words, num_words, pronoun):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and determines the top x number of verbs \n",
    "    (in lemmatized form) most frequently associated with a given pronoun.\n",
    "    Returns a list of x tuples, each containing the count and word associated with the pronoun.\n",
    "    \"\"\"\n",
    "    #pos_tokens = nltk.pos_tag(words)\n",
    "    associated_verbs = list()\n",
    "    verb_counts = dict()\n",
    "    pronouns_plus2 = list()\n",
    "    \n",
    "    # loops through each word, and if the word is the requested pronoun, it adds that word\n",
    "    # plus the next two as a list to a larger list of all instances (so, the result is)\n",
    "    # a list of lists, with each sublist being a set of three words: the pronoun, then the next two words\n",
    "    # in the original text\n",
    "    for index in range(len(words)):\n",
    "        if words[index] == pronoun:\n",
    "            pronouns_plus2.append(words[index:index+3])\n",
    "            \n",
    "    # loops through the list, one set of three words at a time, populating \n",
    "    # a list of the verbs following the pronoun\n",
    "    for pronoun_plus2 in pronouns_plus2:\n",
    "        pos_tagged = nltk.pos_tag(pronoun_plus2)\n",
    "        \n",
    "        # if the word directly after the pronoun is a present/past tense verb, it adds\n",
    "        # the verb in its base form to the list of verbs\n",
    "        if pos_tagged[1][1] in [\"VB\", \"VBD\", \"VBZ\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[1][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "        # if the second word after the pronoun is a participle or gerund, it adds it\n",
    "        # to make sure things like \"she is walking\" have \"walk\" added to the list of verbs\n",
    "        if pos_tagged[2][1] in [\"VBG\", \"VBN\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[2][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "    # loops through the associated verbs and creates a dictionary counting the number of occurrences\n",
    "    for verb in associated_verbs:\n",
    "        if verb not in verb_counts:\n",
    "            verb_counts[verb] = 1\n",
    "        else:\n",
    "            verb_counts[verb] += 1\n",
    "    verb_count_list = list()\n",
    "    for verb,count in verb_counts.items():\n",
    "        verb_count_list.append((count,verb))\n",
    "    verb_count_list.sort(reverse=True)\n",
    "    return verb_count_list[0:num_words]\n",
    "\n",
    "def pronoun_subj_ratio(words, pronoun_subj, pronoun_obj):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens, the subj version of a pronoun (e.g. \"she\"), \n",
    "    and a list of the object versions (e.g. [\"her\", \"hers\"]).\n",
    "    I chose this format because it does not predetermine the available pronouns.\n",
    "    Returns a ratio in decimal form with the subject pronoun as the numerator and \n",
    "    all pronoun counts as the denominator. So, the higher the ratio, the higher frequency of subject pronoun.\n",
    "    \"\"\"\n",
    "    subj_count = 0\n",
    "    obj_count = 0\n",
    "    for word in words:\n",
    "        if word == pronoun_subj:\n",
    "            subj_count += 1\n",
    "        elif word in pronoun_obj:\n",
    "            obj_count += 1\n",
    "    subj_ratio = subj_count / (obj_count + subj_count)\n",
    "    return subj_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses involving collocations.\n",
    "\"\"\"\n",
    "\n",
    "def get_top_collocations(coll_list,num_colls):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations and the number of top results to fetch.\n",
    "    Returns the top num_colls collocations.\n",
    "    \"\"\"\n",
    "    rev_colls = list()\n",
    "    \n",
    "    #rewrite results so they can be sorted by count)\n",
    "    for coll, count in coll_list:\n",
    "        rev_colls.append((count, coll))\n",
    "    rev_colls.sort(reverse=True)\n",
    "    \n",
    "    return rev_colls[0:num_colls]\n",
    "\n",
    "def coll_associations(coll_list, associated_terms, num_results):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations, a list of terms to find associated words for, \n",
    "    and a number of desired top results.\n",
    "    Returns a list of the top num_results words that are collocated with the seeded terms, in lemma form.\n",
    "    \"\"\"\n",
    "    collocated_term_counts = dict()\n",
    "    \n",
    "    associated_lemmas = list()\n",
    "    #turn the list of terms to find collocations for into lemmatized forms\n",
    "    for term in associated_terms:\n",
    "        lemma = lemmatizer.lemmatize(term)\n",
    "        associated_lemmas.append(lemma)\n",
    "\n",
    "    \n",
    "    #loop through the items in the list of collocations to see if either word (in lemma form)\n",
    "    #is in the list of lemmas for which associated collocations are being sought; \n",
    "    #if so, it adds the associated collocation (not the seed term) to a dictionary, and adds the count\n",
    "    for terms, count in coll_list:\n",
    "        term_lemmas = (lemmatizer.lemmatize(terms[0]), lemmatizer.lemmatize(terms[1]))\n",
    "        if term_lemmas[0] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[1]\n",
    "        elif term_lemmas[1] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[0]\n",
    "        else:\n",
    "            associated_lemma = None\n",
    "        #checks to see if associated lemma is in the dictionary already; if not, it sets the count\n",
    "        #to the collocation count; if so, it adds the count to the existing count\n",
    "        if associated_lemma != None and associated_lemma not in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] = count\n",
    "        elif associated_lemma != None and associated_lemma in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] += count\n",
    "            \n",
    "            \n",
    "    count_coll_pairs = list()\n",
    "    for key,value in collocated_term_counts.items():\n",
    "        count_coll_pairs.append((value,key))\n",
    "    count_coll_pairs.sort(reverse=True) \n",
    "    \n",
    "    return count_coll_pairs[0:num_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block collate the file-level and corpus-level analytic functions,\n",
    "as well as the file input-output functions, taking the code from start (folder names representing\n",
    "corpora) to finish (writing out the results to csvs). In a sense, they provide the infrastructure\n",
    "by which the more tool- and task-oriented functions come together to perform a set of analyses\n",
    "and share the results with the user.\n",
    "\"\"\"\n",
    "\n",
    "def file_analysis(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file, tokenizes it, and returns:\n",
    "    1) a dictionary of the results, associating the name of a measure or result with the result,\n",
    "    acquired by running the relevant analytic function.\n",
    "    2) a tuple of the word and sentence tokens for use in the corpus-level analysis.\n",
    "    \"\"\"\n",
    "    doc_results = dict()\n",
    "    text = file_reader(filepath)\n",
    "    word_tokens, sentence_tokens = tokenizers(text)\n",
    "    collocations = get_collocations(word_tokens, stops = False)\n",
    "    doc_results[\"filename\"] = filepath.name\n",
    "    doc_results[\"top 10 words\"] = freq_words(word_tokens,10)\n",
    "    doc_results[\"top 10 bigrams\"] = freq_ngrams(sentence_tokens, 2, 10)\n",
    "    doc_results[\"top 10 verbs_'she'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"she\")\n",
    "    doc_results[\"top 10 verbs_'he'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"he\")\n",
    "    doc_results[\"top 10 verbs_'they'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"they\")\n",
    "    doc_results[\"pronoun ratio_'she'\"] = pronoun_subj_ratio(word_tokens, \"she\", [\"her\",\"hers\"])\n",
    "    doc_results[\"pronoun ratio_'he'\"] = pronoun_subj_ratio(word_tokens, \"he\", [\"his\",\"him\"])\n",
    "    doc_results[\"pronoun ratio_'they'\"] = pronoun_subj_ratio(word_tokens, \"they\", [\"their\",\"theirs\"])  \n",
    "    doc_results[\"top 10 collocations\"] = get_top_collocations(collocations, 10)\n",
    "    \n",
    "    #add collocated related terms results by looping through lists of associated terms\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        doc_results[coll_header] = coll_associations(collocations, associated_wordlists[wordlist_key], 10)\n",
    "\n",
    "    \n",
    "    # this passes the tokens as a result so it can be used in analyzing the corpus as a whole\n",
    "    tokens = (word_tokens, sentence_tokens)\n",
    "    return doc_results, tokens, collocations\n",
    "\n",
    "def process_files(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, acquires a list of all files within it, \n",
    "    analyzes each using file_analysis(), and generates:\n",
    "    1) a list containing the dictionaries full of results for each documents\n",
    "    2) a tuple containing the combined word tokens and combined sentence tokens for the corpus\n",
    "    \"\"\"\n",
    "    file_objs = folder_to_filepaths(folder)\n",
    "    files_results = list()\n",
    "    corpus_word_tokens = list()\n",
    "    corpus_sentence_tokens = list()\n",
    "    corpus_collocations = list()\n",
    "    \n",
    "    # goes through the file path objects in a folder, does file_analysis, adds the dictionary of results\n",
    "    # to a list, and creates lists of all tokens (word and sentence) in the corpus\n",
    "    for file_obj in file_objs:\n",
    "        file_result, file_tokens, file_colls = file_analysis(file_obj)\n",
    "        files_results.append(file_result)\n",
    "        corpus_word_tokens.extend(file_tokens[0])\n",
    "        corpus_sentence_tokens.extend(file_tokens[1])\n",
    "        corpus_collocations.extend(file_colls)\n",
    "    corpus_tokens = (corpus_word_tokens,corpus_sentence_tokens)\n",
    "    \n",
    "    # passes the list of dictionaries and the tuple of corpus tokens for corpus-level analysis\n",
    "    return files_results, corpus_tokens, corpus_collocations\n",
    "\n",
    "def process_folder(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, retrieves the list of results/corpus-level tokens for it\n",
    "    using process_files(), retrieves the results at the corpus level, and appends the corpus-level\n",
    "    results to the list of results, returning this \"complete\" list of results for the corpus and\n",
    "    documents within it.\n",
    "    \"\"\"\n",
    "    documents_results, documents_tokens, documents_collocations = process_files(folder)\n",
    "    print(\"file-level analysis done for corpus: {}\".format(folder))\n",
    "    folder_top10_freq = freq_words(documents_tokens[0], 10)\n",
    "    folder_top10_bigrams = freq_ngrams(documents_tokens[1], 2, 10)\n",
    "    folder_top10_verbs_she = top_pronoun_verb_pairs(documents_tokens[0], 10, \"she\")\n",
    "    folder_top10_verbs_he = top_pronoun_verb_pairs(documents_tokens[0], 10, \"he\")\n",
    "    folder_top10_verbs_they = top_pronoun_verb_pairs(documents_tokens[0], 10, \"they\")\n",
    "    folder_pronoun_ratio_she = pronoun_subj_ratio(documents_tokens[0], \"she\", [\"her\",\"hers\"])\n",
    "    folder_pronoun_ratio_he = pronoun_subj_ratio(documents_tokens[0], \"he\", [\"his\",\"him\"])\n",
    "    folder_pronoun_ratio_they = pronoun_subj_ratio(documents_tokens[0], \"they\", [\"their\",\"theirs\"])\n",
    "    folder_top10_collocations = get_top_collocations(documents_collocations, 10)\n",
    "    \n",
    "    # I realize I could've done this in the above lines, but I wanted to make it clear how\n",
    "    # the corpus analysis uses mostly different or modified functions for the results and adds that\n",
    "    folder_result = {\"filename\": folder, \"top 10 words\": folder_top10_freq, \n",
    "                     \"top 10 bigrams\": folder_top10_bigrams, \n",
    "                     \"top 10 verbs_'she'\": folder_top10_verbs_she, \n",
    "                     \"top 10 verbs_'he'\": folder_top10_verbs_he,\n",
    "                     \"top 10 verbs_'they'\": folder_top10_verbs_they,\n",
    "                     \"pronoun ratio_'she'\": folder_pronoun_ratio_she, \n",
    "                     \"pronoun ratio_'he'\": folder_pronoun_ratio_he,\n",
    "                     \"pronoun ratio_'they'\": folder_pronoun_ratio_they,\n",
    "                     \"top 10 collocations\": folder_top10_collocations}\n",
    "\n",
    "    coll_associations_results = dict()\n",
    "    #add collocated related terms results by looping through lists of associated terms, creating dictionary\n",
    "    #so that the specific name can be written out in the header row for the csv\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        folder_result[coll_header] = coll_associations(documents_collocations, \n",
    "                                                                    associated_wordlists[wordlist_key], \n",
    "                                                                    10)\n",
    "    \n",
    "    print(\"corpus-level analysis done for corpus: {}\".format(folder))\n",
    "    documents_results.append(folder_result)\n",
    "    return documents_results\n",
    "\n",
    "def write_results(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name, runs process_folder() on it (which in turn performs the analyses per \n",
    "    document and corpus, and collates the results into a list of dictionaries), creates the\n",
    "    results csv for the folder with a header row, and then writes each dictionary of results\n",
    "    as a row.\n",
    "    \"\"\"\n",
    "    corpus_results = process_folder(folder)\n",
    "    header = list(corpus_results[0].keys())\n",
    "    csv_creator(folder,header)\n",
    "    \n",
    "    # note that this for loop goes through the list of dictionaries of results and, for each one,\n",
    "    # writes them out to the csv, line by line\n",
    "    for document_result in corpus_results:\n",
    "        csv_writer(folder, document_result)\n",
    "    print(\"results written out to csv for corpus: {}\\n--------------------\".format(folder))\n",
    "    #added this so I can do some analytics as seen at the end of this notebook\n",
    "    return corpus_results\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Iterates through the corpora as defined at the top, which should reflect the folder names in the\n",
    "    \"data\" folder, and runs write_results() on it (which in turn processes the files, acquires the\n",
    "    results, and writes the results out to a csv file).\n",
    "    \"\"\"\n",
    "    global res_dict\n",
    "    res_dict = dict()\n",
    "    for corpus in corpora:\n",
    "        res = write_results(corpus)\n",
    "        res_dict[corpus] = res\n",
    "    print(\"done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-level analysis done for corpus: lesbian-fanfic\n",
      "corpus-level analysis done for corpus: lesbian-fanfic\n",
      "results written out to csv for corpus: lesbian-fanfic\n",
      "--------------------\n",
      "file-level analysis done for corpus: lesbian-pulp\n",
      "corpus-level analysis done for corpus: lesbian-pulp\n",
      "results written out to csv for corpus: lesbian-pulp\n",
      "--------------------\n",
      "done!!\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesbian-fanfic\n",
      "\n",
      "top 10 words\n",
      "[(10, 'like'), (10, 'back'), (9, 'just'), (9, \"don't\"), (8, \"i'm\"), (7, 'said'), (5, 'know'), (5, \"it's\"), (4, \"didn't\"), (3, 'eyes')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'she'\n",
      "[(11, 'be'), (10, 'have'), (9, 'say'), (8, 'do'), (7, \"didn't\"), (5, 'want'), (5, 'look'), (5, 'know'), (4, 'go'), (4, 'felt'), (3, 'whisper'), (3, 'take'), (3, 'get'), (3, \"couldn't\")]\n",
      "----------------------------------------\n",
      "top 10 verbs_'he'\n",
      "[(10, 'be'), (8, 'say'), (8, 'have'), (5, 'want'), (5, 'turn'), (5, 'go'), (5, 'get'), (5, 'do'), (5, \"didn't\"), (4, 'think'), (3, 'sigh'), (3, 'look')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'they'\n",
      "[(11, 'be'), (9, 'have'), (8, 'do'), (6, 'talk'), (5, 'make'), (5, 'go'), (5, 'get'), (4, 'think'), (3, 'want'), (3, 'walk'), (3, 'sit'), (3, 'kiss')]\n",
      "----------------------------------------\n",
      "top 10 coll - body\n",
      "[(8, 'around'), (5, 'upper'), (4, 'trembled'), (4, 'bottom'), (3, 'folded'), (3, 'curled')]\n",
      "----------------------------------------\n",
      "top 10 coll - intimacy\n",
      "[(4, \"i'd\")]\n",
      "----------------------------------------\n",
      "top 10 coll - identity\n",
      "[(3, 'assured')]\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "lesbian-pulp\n",
      "\n",
      "top 10 words\n",
      "[(11, 'said'), (10, 'like'), (10, \"don't\"), (9, 'know'), (5, 'laura'), (5, \"didn't\"), (4, 'time'), (4, 'thought'), (4, 'beebo'), (3, 'just'), (3, 'jack'), (3, \"i'm\"), (3, 'go'), (3, 'erika'), (3, 'beth')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'she'\n",
      "[(11, 'say'), (11, 'have'), (11, \"didn't\"), (11, 'be'), (10, 'think'), (9, 'look'), (9, 'know'), (9, 'felt'), (7, 'go'), (6, 'want'), (5, \"couldn't\")]\n",
      "----------------------------------------\n",
      "top 10 verbs_'he'\n",
      "[(11, 'say'), (11, 'look'), (11, 'have'), (11, 'be'), (7, 'think'), (7, \"didn't\"), (6, 'want'), (6, 'smile'), (6, 'know'), (6, 'do'), (5, 'go'), (4, 'turn'), (4, 'take'), (4, 'ask'), (3, 'get')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'they'\n",
      "[(11, 'have'), (11, 'be'), (9, 'look'), (9, 'go'), (8, 'walk'), (7, 'sit'), (7, 'get'), (7, 'do'), (6, 'make'), (5, 'stand'), (4, 'take'), (3, 'meet'), (3, 'come')]\n",
      "----------------------------------------\n",
      "top 10 coll - body\n",
      "[(11, 'around'), (7, 'one'), (6, 'whole'), (4, 'long'), (4, \"laura's\"), (4, 'dark'), (4, 'black'), (3, \"erika's\"), (3, \"beth's\"), (3, \"beebo's\")]\n",
      "----------------------------------------\n",
      "top 10 coll - intimacy\n",
      "[(10, 'make'), (9, 'made'), (8, 'making'), (5, 'really'), (5, 'real'), (4, 'still'), (4, 'even'), (3, 'laura'), (3, 'could'), (3, 'beebo')]\n",
      "----------------------------------------\n",
      "top 10 coll - identity\n",
      "[(5, 'bar'), (4, 'pity'), (4, 'conscious'), (3, 'laura'), (3, \"i'm\"), (3, 'girl'), (3, 'consciously')]\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The functions in this block run some basic analytics on results to help me\n",
    "find inroads for my analysis. It's mostly just to help me and I wouldn't include it were I to share\n",
    "this code, but I am temporarily including it here.\n",
    "\"\"\"\n",
    "commonalities = dict()\n",
    "\n",
    "for text in res_dict[\"lesbian-fanfic\"]:\n",
    "    for result in text:\n",
    "        commonalities[result] = None\n",
    "        \n",
    "\n",
    "\n",
    "def read_csv(corpus_name):\n",
    "    filename = \"results_{}.csv\".format(corpus_name)\n",
    "    filepath = pathlib.Path(filename)\n",
    "    corpus_rows = list()\n",
    "    with open(filepath,\"r\") as file_obj:\n",
    "        content = file_obj.read()\n",
    "    rows = content.split(\"\\n\")\n",
    "    corpus_rows.extend(rows[0:-1])\n",
    "    return corpus_rows\n",
    "\n",
    "\n",
    "\n",
    "for corpus in corpora:\n",
    "    csv_dict = dict()\n",
    "    rows = read_csv(corpus)\n",
    "    csv_dict[\"corpus\"] = corpus\n",
    "    column_heads = rows[0].split(\",\")\n",
    "    relevant_headers = column_heads[1:2]+column_heads[3:6]+column_heads[9:]\n",
    "    for column_head in relevant_headers:\n",
    "        csv_dict[column_head] = None\n",
    "    for row in rows[1:]:\n",
    "        split_row = row.split(\",\")\n",
    "        split_row = split_row[1:6]+split_row[9:]\n",
    "        split_row.remove(split_row[1])\n",
    "        for column_ind in range(len(split_row)):\n",
    "            result = []\n",
    "            for word in split_row[column_ind].split():\n",
    "                result.append(word)\n",
    "            if csv_dict[relevant_headers[column_ind]] == None:\n",
    "                csv_dict[relevant_headers[column_ind]] = result\n",
    "            else:\n",
    "                csv_dict[relevant_headers[column_ind]].extend(result)\n",
    "    print(corpus+\"\\n\")\n",
    "    for relevant_header in relevant_headers:\n",
    "        if relevant_header != \"top 10 collocations\":\n",
    "            print(relevant_header)\n",
    "            counts = {}\n",
    "            counts_bynum = []\n",
    "            res_list = csv_dict[relevant_header]\n",
    "            for word in res_list:\n",
    "                if word not in counts:\n",
    "                    counts[word] = 1\n",
    "                else:\n",
    "                    counts[word] += 1\n",
    "            for key in counts:\n",
    "                if counts[key] >= 3:\n",
    "                    counts_bynum.append((counts[key],key))\n",
    "            counts_bynum.sort(reverse=True)\n",
    "            print(counts_bynum)\n",
    "            print(\"----------------------------------------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09585121948758317\n",
      "0.13015177781220566\n",
      "0.06094546127367356\n",
      "0.12893410005570138\n",
      "0.09007824494125866\n",
      "0.09610399392274392\n",
      "0.109493932186128\n",
      "0.15379715808882483\n",
      "0.07661534789354335\n",
      "0.06527732776959588\n",
      "0.015851902726902733\n",
      "0.01768262987012987\n",
      "0.01836577522004063\n",
      "0.002069600074784402\n",
      "0.08328849279228069\n",
      "0.08438627240008051\n",
      "0.09423920101637495\n",
      "0.017504772587927087\n",
      "0.14148872783351463\n",
      "0.10472881576873165\n"
     ]
    }
   ],
   "source": [
    "working_doc = \"data/lesbian-pulp/bannon_odd-girl-out.txt\"\n",
    "with open(working_doc,\"r\",encoding=\"utf-8\") as f:\n",
    "    txt = f.read().strip().lower()\n",
    "    \n",
    "from textblob import TextBlob\n",
    "    \n",
    "def chapter_segmenter(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    chapters = list()\n",
    "    chapter_pattern = \"chapter [a-z0-9:\\.]+(?=\\n)\"\n",
    "    chapter_headers = re.findall(chapter_pattern,text)\n",
    "    if len(chapter_headers) > 2:\n",
    "        counter = 0\n",
    "        while counter < len(chapter_headers):\n",
    "            if counter < len(chapter_headers) - 1:\n",
    "                chapter_text_pattern = \"(?<={}\\n)[\\s\\S]+(?={}\\n)\".format(chapter_headers[counter],chapter_headers[counter+1])\n",
    "                chapter_text = re.search(chapter_text_pattern,text)\n",
    "                try:\n",
    "                    chapters.append(chapter_text.group())\n",
    "                except:\n",
    "                    chapters.append(chapter_text)\n",
    "            else:\n",
    "                chapter_text_pattern = \"{}[\\s\\S]+\".format(chapter_headers[counter])\n",
    "                chapter_text = re.search(chapter_text_pattern,text)\n",
    "                chapters.append(chapter_text.group())\n",
    "            counter += 1\n",
    "    else:\n",
    "        chapters = list()\n",
    "        window = int(len(text)/15)\n",
    "        while len(chapters) < 15:\n",
    "            if len(chapters) < 14:\n",
    "                ind = window * len(chapters)\n",
    "                next_window = ind + window\n",
    "                newline_ind = text.rfind(\"\\n\",ind,next_window)\n",
    "                chapter = text[ind:next_window]\n",
    "                chapters.append(chapter)\n",
    "                ind = newline_ind+1\n",
    "            else:\n",
    "                chapters.append(text[ind:])\n",
    "\n",
    "    return chapters\n",
    "            \n",
    "res = chapter_segmenter(txt)\n",
    "for result in res:\n",
    "    tb1 = TextBlob(result)\n",
    "    print(tb1.sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
