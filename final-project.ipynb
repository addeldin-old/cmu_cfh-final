{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pathlib\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(\"[\\wâ€™'']+\")\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "#this should change based on the folder names for each corpus held in the \"data\" folder\n",
    "corpora = [\"lesbian-fanfic\",\"lesbian-pulp\"]\n",
    "\n",
    "#populate with lists of words that are associated with each other\n",
    "associated_wordlists = {\n",
    "    \"body\": [\"body\", \"breasts\", \"hips\", \"lipstick\", \"hair\", \"brow\", \"arms\", \"legs\", \"lips\",\"waist\"],\n",
    "    \"intimacy\": [\"intimacy\",\"tenderness\", \"tender\", \"loving\", \"warm\", \"warmth\", \"ache\", \"touch\",\"touching\",\"love\"],\n",
    "    \"identity\": [\"lesbian\",\"homosexual\",\"gay\",\"queer\",\"bisexual\",\"self\",\"herself\"]\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are \"utility functions,\" which is to say they perform a task in service\n",
    "of some other core analytic function, e.g. removing punctuatons, getting function words, or reformatting\n",
    "results like ngrams.\n",
    "\"\"\"\n",
    "def tokenizers(text):\n",
    "    \"\"\"\n",
    "    Receives a string and returns a list of the word tokens and sentence tokens, the\n",
    "    sentence tokens being for the purpose of ngram analysis.\n",
    "    \"\"\"\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "    sent_tokens = sent_tokenize(text)\n",
    "    return word_tokens, sent_tokens\n",
    "\n",
    "def get_function_words():\n",
    "    \"\"\"\n",
    "    When run, returns a list of function words that is based on a txt file stored in the \"data\" folder.\n",
    "    \"\"\"\n",
    "    filepath = pathlib.Path(\"data/function_words.txt\")\n",
    "    fin = open(filepath)\n",
    "    result = list()                            \n",
    "    for line in fin:\n",
    "        result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Receives a string and strips any punctuation that is part of string.punctuation, returning\n",
    "    the punctuationless string.\n",
    "    \"\"\"\n",
    "    text_nopunct = text.translate(text.maketrans('', '', punctuation))\n",
    "    return text_nopunct\n",
    "\n",
    "def filter_stops(w):\n",
    "    \"\"\"Receives a string and filters out the word if it is less than three characters or a stop word.\n",
    "    Returns the word if it passes the criteria and nothing if the word does not.\n",
    "    This is to be used as a filter for the collocations NLTK tool.\"\"\"\n",
    "    if len(w) < 3 or w in english_stops:\n",
    "        return w\n",
    "    return None\n",
    "\n",
    "def freq_result_rewriter(raw_result,column_head):\n",
    "    \"\"\"\n",
    "    In service of writing out results to a csv without extra commas breaking up the results,\n",
    "    this function receives a list of tuples containing the frequency and word(s) for some kind of\n",
    "    \"top x words/ngrams\" result, and returns a string of the words or ngrams separated by spaces.\n",
    "    If the result is a list of ngram counts, then it puts the words in the ngram within single quotes\n",
    "    to avoid a lack of clarity of where each ngram starts and ends.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    \n",
    "    # the least indented if/else determines if it's a \"normal\" result or if the result is itself\n",
    "    # a list of results, which would then need to converted from frequency-phrase tuples to a string of\n",
    "    # the phrases\n",
    "    if re.search(\"^top [0-9]+\",column_head) != None:\n",
    "        \n",
    "        # this if/else checks if it's an ngram, because then the phrase is multiple words and needs\n",
    "        # to be converted to a string of those words separated by spaces and surrounded by single quotes\n",
    "        if re.search(\"grams$\",column_head) != None or re.search(\"collocations$\",column_head) != None:\n",
    "            \n",
    "            # this for loop goes through the ngram results, grabs the ngram tuple, \n",
    "            # and converts it into a string surrounded by single quotes, then adds them to a larger\n",
    "            # result string\n",
    "            for ngram_result_index in range(len(raw_result)):\n",
    "                gram_tuple = raw_result[ngram_result_index][1]\n",
    "                gram_list = list(gram_tuple)\n",
    "                gram = \" \".join(gram_list)\n",
    "                gram = \"'{}'\".format(gram)\n",
    "                \n",
    "                # this if/else ensures there is not an extra space after the last word\n",
    "                if ngram_result_index < len(raw_result)-1:\n",
    "                    result += gram+\" \"\n",
    "                else:\n",
    "                    result += gram\n",
    "        else:\n",
    "            \n",
    "            # this for loop goes through a non-ngram frequency-phrase result and fetches the phrase,\n",
    "            # adding it to a larger string of the results separated by spaces\n",
    "            for word_result_index in range(len(raw_result)):\n",
    "                word = raw_result[word_result_index][1]\n",
    "                if word_result_index < len(raw_result)-1:\n",
    "                        result += word+\" \"\n",
    "                else:\n",
    "                    result += word   \n",
    "                    \n",
    "    else:\n",
    "        result = str(raw_result)\n",
    "    return result\n",
    "\n",
    "def get_collocations(words,stops=None):\n",
    "    \"\"\"\n",
    "    Uses NLTK's bigram collocation finder to receive a list of words (and whether or not to filter stop words)\n",
    "    and find all bigram collocations that appear at least three times.\n",
    "    Returns a list of tuples that each contain two items: a tuple of the two collocated words, and a number\n",
    "    representing the number of appearances.\n",
    "    \"\"\"    \n",
    "    bcf = BigramCollocationFinder.from_words(words)\n",
    "    if stops == False:\n",
    "        bcf.apply_word_filter(filter_stops)\n",
    "    elif stops == True:\n",
    "        pass\n",
    "    bcf.apply_freq_filter(2)  \n",
    "    result = list(bcf.ngram_fd.items())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block are file-oriented, which is to say they are involved in converting\n",
    "strings to file objects (e.g. a folder to a list of filepaths), or in writing out results to a\n",
    "csv file.\n",
    "\"\"\"\n",
    "\n",
    "def folder_to_filepaths(folder):\n",
    "    \"\"\"\n",
    "    Receives the name of a folder within a subfolder called \"data\" and returns a list of\n",
    "    Path objects, one for each file within the folder.\n",
    "    \"\"\"\n",
    "    folder_path = pathlib.Path(\"data/{}\".format(folder))\n",
    "    files = folder_path.glob('*.txt')\n",
    "    file_names = list()\n",
    "    for file in files:\n",
    "        file_names.append(file)\n",
    "    return file_names\n",
    "\n",
    "def file_reader(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file and returns a string of the text in that file.\n",
    "    \"\"\"\n",
    "    with open(filepath,encoding=\"utf-8\") as fin:\n",
    "        f_text = fin.read().strip().lower()\n",
    "    return f_text\n",
    "\n",
    "def csv_creator(folder_name,header_items):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a list of the column names,\n",
    "    and then creates a file for that corpus with the header row written in.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"w\") as csv_out:\n",
    "        counter = 0\n",
    "        for header_item in header_items:\n",
    "            if counter < len(header_items)-1:\n",
    "                csv_out.write(header_item+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(header_item+\"\\n\")\n",
    "\n",
    "def csv_writer(folder_name,results):\n",
    "    \"\"\"\n",
    "    Receives the folder name for the corpus being analyzed and a dictionary of results for \n",
    "    a document or corpus, and then writes a new row into the respective corpus results csv \n",
    "    with that document's or corpus's results. Note that checks to see if the result is a \n",
    "    \"top\" number of words (e.g. most frequent words, most frequent ngrams), and calls\n",
    "    the function that reformats the result if so to make sure there are not extra \n",
    "    commas while writing out the result.\n",
    "    \"\"\"\n",
    "    csv_name = \"results_{}.csv\".format(folder_name)\n",
    "    csv_path = pathlib.Path(csv_name)\n",
    "    with open(csv_path,\"a\") as csv_out:\n",
    "        counter = 0\n",
    "        \n",
    "        # this for loop goes through the dictionary of results, fetches the value (so an individual result),\n",
    "        # and then formats the result\n",
    "        for key in results:\n",
    "            unformatted_result = results[key]\n",
    "            result = freq_result_rewriter(unformatted_result,key)\n",
    "            \n",
    "            # this if/else ensures that the line ends with a newline, not a comma\n",
    "            if counter < len(results)-1:\n",
    "                csv_out.write(result+\",\")\n",
    "                counter += 1\n",
    "            else:\n",
    "                csv_out.write(result+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on finding common words or sequences of words.\n",
    "\"\"\"\n",
    "\n",
    "def freq_words(words, num_words):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and the number of \"top words\" to find (let's call it x),\n",
    "    and determines the x most frequently occurring words.\n",
    "    Returns a list of x tuples, with each containing the count and word associated with it.\n",
    "    \"\"\"\n",
    "    function_words = get_function_words()\n",
    "    word_counts = dict()\n",
    "    \n",
    "    # goes through the tokens list, creating a dictionary that relates the\n",
    "    # word to how many times it appears, so long as it is not a function word\n",
    "    for word in words:\n",
    "        if word not in function_words:\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "        else:\n",
    "            pass\n",
    "    count_word_pairs = list()\n",
    "    \n",
    "    # converts the dictionary word-count relationships to a tuple with the count first, then the word\n",
    "    for key, value in word_counts.items():\n",
    "        count_word_pairs.append((value,key))\n",
    "    count_word_pairs.sort(reverse=True)\n",
    "    return count_word_pairs[0:num_words]\n",
    "\n",
    "def freq_ngrams(sentences, n, num_ngrams):\n",
    "    \"\"\"\n",
    "    Receives a list of sentence tokens, the n ngrams to find, and the number of \"top ngrams\" to find (x),\n",
    "    and determines the x most frequently occuring ngrams, without traversing sentences.\n",
    "    Returns a list of x tuples, with each containing the count and \n",
    "    a tuple of the words in the ngram associated with it.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    \n",
    "    # goes through each sentence, tokenizes it, and then gathers every ngram for the sentence of size n\n",
    "    for sent in sentences:\n",
    "        words = tokenizer.tokenize(sent)\n",
    "        ngram_obj = nltk.ngrams(words, n)\n",
    "        sent_ngrams = list(ngram_obj)\n",
    "        ngrams.extend(sent_ngrams)\n",
    "    ngram_counts = dict()\n",
    "    \n",
    "    # goes through each ngram and creates a dictionary relating the ngram to how many times it appears\n",
    "    for ngram in ngrams:\n",
    "        if ngram not in ngram_counts:\n",
    "            ngram_counts[ngram] = 1\n",
    "        else:\n",
    "            ngram_counts[ngram] += 1\n",
    "    count_ngram_pairs = list()\n",
    "    for key,value in ngram_counts.items():\n",
    "        count_ngram_pairs.append((value,key))\n",
    "    count_ngram_pairs.sort(reverse=True)\n",
    "    \n",
    "    # this if/else ensures there won't be an error if the number of ngrams is less than the requested number\n",
    "    if len(count_ngram_pairs) < num_ngrams:\n",
    "        return count_ngram_pairs\n",
    "    else:\n",
    "        return count_ngram_pairs[0:num_ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses around pronouns.\n",
    "\"\"\"\n",
    "\n",
    "def top_pronoun_verb_pairs(words, num_words, pronoun):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens and determines the top x number of verbs \n",
    "    (in lemmatized form) most frequently associated with a given pronoun.\n",
    "    Returns a list of x tuples, each containing the count and word associated with the pronoun.\n",
    "    \"\"\"\n",
    "    #pos_tokens = nltk.pos_tag(words)\n",
    "    associated_verbs = list()\n",
    "    verb_counts = dict()\n",
    "    pronouns_plus2 = list()\n",
    "    \n",
    "    # loops through each word, and if the word is the requested pronoun, it adds that word\n",
    "    # plus the next two as a list to a larger list of all instances (so, the result is)\n",
    "    # a list of lists, with each sublist being a set of three words: the pronoun, then the next two words\n",
    "    # in the original text\n",
    "    for index in range(len(words)):\n",
    "        if words[index] == pronoun:\n",
    "            pronouns_plus2.append(words[index:index+3])\n",
    "            \n",
    "    # loops through the list, one set of three words at a time, populating \n",
    "    # a list of the verbs following the pronoun\n",
    "    for pronoun_plus2 in pronouns_plus2:\n",
    "        pos_tagged = nltk.pos_tag(pronoun_plus2)\n",
    "        \n",
    "        # if the word directly after the pronoun is a present/past tense verb, it adds\n",
    "        # the verb in its base form to the list of verbs\n",
    "        if pos_tagged[1][1] in [\"VB\", \"VBD\", \"VBZ\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[1][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "        # if the second word after the pronoun is a participle or gerund, it adds it\n",
    "        # to make sure things like \"she is walking\" have \"walk\" added to the list of verbs\n",
    "        if pos_tagged[2][1] in [\"VBG\", \"VBN\"]:\n",
    "            lemma_form = lemmatizer.lemmatize(pos_tagged[2][0],\"v\")\n",
    "            associated_verbs.append(lemma_form)\n",
    "            \n",
    "    # loops through the associated verbs and creates a dictionary counting the number of occurrences\n",
    "    for verb in associated_verbs:\n",
    "        if verb not in verb_counts:\n",
    "            verb_counts[verb] = 1\n",
    "        else:\n",
    "            verb_counts[verb] += 1\n",
    "    verb_count_list = list()\n",
    "    for verb,count in verb_counts.items():\n",
    "        verb_count_list.append((count,verb))\n",
    "    verb_count_list.sort(reverse=True)\n",
    "    return verb_count_list[0:num_words]\n",
    "\n",
    "def pronoun_subj_ratio(words, pronoun_subj, pronoun_obj):\n",
    "    \"\"\"\n",
    "    Receives a list of word tokens, the subj version of a pronoun (e.g. \"she\"), \n",
    "    and a list of the object versions (e.g. [\"her\", \"hers\"]).\n",
    "    I chose this format because it does not predetermine the available pronouns.\n",
    "    Returns a ratio in decimal form with the subject pronoun as the numerator and \n",
    "    all pronoun counts as the denominator. So, the higher the ratio, the higher frequency of subject pronoun.\n",
    "    \"\"\"\n",
    "    subj_count = 0\n",
    "    obj_count = 0\n",
    "    for word in words:\n",
    "        if word == pronoun_subj:\n",
    "            subj_count += 1\n",
    "        elif word in pronoun_obj:\n",
    "            obj_count += 1\n",
    "    subj_ratio = subj_count / (obj_count + subj_count)\n",
    "    return subj_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block perform the analyses that will be\n",
    "used in the file and folder processing (or top words/ngrams measures at the corpus level)\n",
    "and ultimately written out to the results csvs. They focus on analyses involving collocations.\n",
    "\"\"\"\n",
    "\n",
    "def get_top_collocations(coll_list,num_colls):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations and the number of top results to fetch.\n",
    "    Returns the top num_colls collocations.\n",
    "    \"\"\"\n",
    "    rev_colls = list()\n",
    "    \n",
    "    #rewrite results so they can be sorted by count)\n",
    "    for coll, count in coll_list:\n",
    "        rev_colls.append((count, coll))\n",
    "    rev_colls.sort(reverse=True)\n",
    "    \n",
    "    return rev_colls[0:num_colls]\n",
    "\n",
    "def coll_associations(coll_list, associated_terms, num_results):\n",
    "    \"\"\"\n",
    "    Receives a list of collocations, a list of terms to find associated words for, \n",
    "    and a number of desired top results.\n",
    "    Returns a list of the top num_results words that are collocated with the seeded terms, in lemma form.\n",
    "    \"\"\"\n",
    "    collocated_term_counts = dict()\n",
    "    \n",
    "    associated_lemmas = list()\n",
    "    #turn the list of terms to find collocations for into lemmatized forms\n",
    "    for term in associated_terms:\n",
    "        lemma = lemmatizer.lemmatize(term)\n",
    "        associated_lemmas.append(lemma)\n",
    "\n",
    "    \n",
    "    #loop through the items in the list of collocations to see if either word (in lemma form)\n",
    "    #is in the list of lemmas for which associated collocations are being sought; \n",
    "    #if so, it adds the associated collocation (not the seed term) to a dictionary, and adds the count\n",
    "    for terms, count in coll_list:\n",
    "        term_lemmas = (lemmatizer.lemmatize(terms[0]), lemmatizer.lemmatize(terms[1]))\n",
    "        if term_lemmas[0] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[1]\n",
    "        elif term_lemmas[1] in associated_lemmas:\n",
    "            associated_lemma = term_lemmas[0]\n",
    "        else:\n",
    "            associated_lemma = None\n",
    "        #checks to see if associated lemma is in the dictionary already; if not, it sets the count\n",
    "        #to the collocation count; if so, it adds the count to the existing count\n",
    "        if associated_lemma != None and associated_lemma not in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] = count\n",
    "        elif associated_lemma != None and associated_lemma in collocated_term_counts:\n",
    "            collocated_term_counts[associated_lemma] += count\n",
    "            \n",
    "            \n",
    "    count_coll_pairs = list()\n",
    "    for key,value in collocated_term_counts.items():\n",
    "        count_coll_pairs.append((value,key))\n",
    "    count_coll_pairs.sort(reverse=True) \n",
    "    \n",
    "    return count_coll_pairs[0:num_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions in this block collate the file-level and corpus-level analytic functions,\n",
    "as well as the file input-output functions, taking the code from start (folder names representing\n",
    "corpora) to finish (writing out the results to csvs). In a sense, they provide the infrastructure\n",
    "by which the more tool- and task-oriented functions come together to perform a set of analyses\n",
    "and share the results with the user.\n",
    "\"\"\"\n",
    "\n",
    "def file_analysis(filepath):\n",
    "    \"\"\"\n",
    "    Receives a Path object for a file, tokenizes it, and returns:\n",
    "    1) a dictionary of the results, associating the name of a measure or result with the result,\n",
    "    acquired by running the relevant analytic function.\n",
    "    2) a tuple of the word and sentence tokens for use in the corpus-level analysis.\n",
    "    \"\"\"\n",
    "    doc_results = dict()\n",
    "    text = file_reader(filepath)\n",
    "    word_tokens, sentence_tokens = tokenizers(text)\n",
    "    collocations = get_collocations(word_tokens, stops = False)\n",
    "    doc_results[\"filename\"] = filepath.name\n",
    "    doc_results[\"top 10 words\"] = freq_words(word_tokens,10)\n",
    "    doc_results[\"top 10 bigrams\"] = freq_ngrams(sentence_tokens, 2, 10)\n",
    "    doc_results[\"top 10 verbs_'she'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"she\")\n",
    "    doc_results[\"top 10 verbs_'he'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"he\")\n",
    "    doc_results[\"top 10 verbs_'they'\"] = top_pronoun_verb_pairs(word_tokens, 10, \"they\")\n",
    "    doc_results[\"pronoun ratio_'she'\"] = pronoun_subj_ratio(word_tokens, \"she\", [\"her\",\"hers\"])\n",
    "    doc_results[\"pronoun ratio_'he'\"] = pronoun_subj_ratio(word_tokens, \"he\", [\"his\",\"him\"])\n",
    "    doc_results[\"pronoun ratio_'they'\"] = pronoun_subj_ratio(word_tokens, \"they\", [\"their\",\"theirs\"])  \n",
    "    doc_results[\"top 10 collocations\"] = get_top_collocations(collocations, 10)\n",
    "    \n",
    "    #add collocated related terms results by looping through lists of associated terms\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        doc_results[coll_header] = coll_associations(collocations, associated_wordlists[wordlist_key], 10)\n",
    "\n",
    "    \n",
    "    # this passes the tokens as a result so it can be used in analyzing the corpus as a whole\n",
    "    tokens = (word_tokens, sentence_tokens)\n",
    "    return doc_results, tokens, collocations\n",
    "\n",
    "def process_files(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, acquires a list of all files within it, \n",
    "    analyzes each using file_analysis(), and generates:\n",
    "    1) a list containing the dictionaries full of results for each documents\n",
    "    2) a tuple containing the combined word tokens and combined sentence tokens for the corpus\n",
    "    \"\"\"\n",
    "    file_objs = folder_to_filepaths(folder)\n",
    "    files_results = list()\n",
    "    corpus_word_tokens = list()\n",
    "    corpus_sentence_tokens = list()\n",
    "    corpus_collocations = list()\n",
    "    \n",
    "    # goes through the file path objects in a folder, does file_analysis, adds the dictionary of results\n",
    "    # to a list, and creates lists of all tokens (word and sentence) in the corpus\n",
    "    for file_obj in file_objs:\n",
    "        file_result, file_tokens, file_colls = file_analysis(file_obj)\n",
    "        files_results.append(file_result)\n",
    "        corpus_word_tokens.extend(file_tokens[0])\n",
    "        corpus_sentence_tokens.extend(file_tokens[1])\n",
    "        corpus_collocations.extend(file_colls)\n",
    "    corpus_tokens = (corpus_word_tokens,corpus_sentence_tokens)\n",
    "    \n",
    "    # passes the list of dictionaries and the tuple of corpus tokens for corpus-level analysis\n",
    "    return files_results, corpus_tokens, corpus_collocations\n",
    "\n",
    "def process_folder(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name for a corpus, retrieves the list of results/corpus-level tokens for it\n",
    "    using process_files(), retrieves the results at the corpus level, and appends the corpus-level\n",
    "    results to the list of results, returning this \"complete\" list of results for the corpus and\n",
    "    documents within it.\n",
    "    \"\"\"\n",
    "    documents_results, documents_tokens, documents_collocations = process_files(folder)\n",
    "    print(\"file-level analysis done for corpus: {}\".format(folder))\n",
    "    folder_top10_freq = freq_words(documents_tokens[0], 10)\n",
    "    folder_top10_bigrams = freq_ngrams(documents_tokens[1], 2, 10)\n",
    "    folder_top10_verbs_she = top_pronoun_verb_pairs(documents_tokens[0], 10, \"she\")\n",
    "    folder_top10_verbs_he = top_pronoun_verb_pairs(documents_tokens[0], 10, \"he\")\n",
    "    folder_top10_verbs_they = top_pronoun_verb_pairs(documents_tokens[0], 10, \"they\")\n",
    "    folder_pronoun_ratio_she = pronoun_subj_ratio(documents_tokens[0], \"she\", [\"her\",\"hers\"])\n",
    "    folder_pronoun_ratio_he = pronoun_subj_ratio(documents_tokens[0], \"he\", [\"his\",\"him\"])\n",
    "    folder_pronoun_ratio_they = pronoun_subj_ratio(documents_tokens[0], \"they\", [\"their\",\"theirs\"])\n",
    "    folder_top10_collocations = get_top_collocations(documents_collocations, 10)\n",
    "    \n",
    "    # I realize I could've done this in the above lines, but I wanted to make it clear how\n",
    "    # the corpus analysis uses mostly different or modified functions for the results and adds that\n",
    "    folder_result = {\"filename\": folder, \"top 10 words\": folder_top10_freq, \n",
    "                     \"top 10 bigrams\": folder_top10_bigrams, \n",
    "                     \"top 10 verbs_'she'\": folder_top10_verbs_she, \n",
    "                     \"top 10 verbs_'he'\": folder_top10_verbs_he,\n",
    "                     \"top 10 verbs_'they'\": folder_top10_verbs_they,\n",
    "                     \"pronoun ratio_'she'\": folder_pronoun_ratio_she, \n",
    "                     \"pronoun ratio_'he'\": folder_pronoun_ratio_he,\n",
    "                     \"pronoun ratio_'they'\": folder_pronoun_ratio_they,\n",
    "                     \"top 10 collocations\": folder_top10_collocations}\n",
    "\n",
    "    coll_associations_results = dict()\n",
    "    #add collocated related terms results by looping through lists of associated terms, creating dictionary\n",
    "    #so that the specific name can be written out in the header row for the csv\n",
    "    for wordlist_key in associated_wordlists:\n",
    "        coll_header = \"top 10 coll - {}\".format(wordlist_key)\n",
    "        folder_result[coll_header] = coll_associations(documents_collocations, \n",
    "                                                                    associated_wordlists[wordlist_key], \n",
    "                                                                    10)\n",
    "    \n",
    "    print(\"corpus-level analysis done for corpus: {}\".format(folder))\n",
    "    documents_results.append(folder_result)\n",
    "    return documents_results\n",
    "\n",
    "def write_results(folder):\n",
    "    \"\"\"\n",
    "    Receives a folder name, runs process_folder() on it (which in turn performs the analyses per \n",
    "    document and corpus, and collates the results into a list of dictionaries), creates the\n",
    "    results csv for the folder with a header row, and then writes each dictionary of results\n",
    "    as a row.\n",
    "    \"\"\"\n",
    "    corpus_results = process_folder(folder)\n",
    "    header = list(corpus_results[0].keys())\n",
    "    csv_creator(folder,header)\n",
    "    \n",
    "    # note that this for loop goes through the list of dictionaries of results and, for each one,\n",
    "    # writes them out to the csv, line by line\n",
    "    for document_result in corpus_results:\n",
    "        csv_writer(folder, document_result)\n",
    "    print(\"results written out to csv for corpus: {}\\n--------------------\".format(folder))\n",
    "    #added this so I can do some analytics as seen at the end of this notebook\n",
    "    return corpus_results\n",
    "\n",
    "def run():\n",
    "    \"\"\"\n",
    "    Iterates through the corpora as defined at the top, which should reflect the folder names in the\n",
    "    \"data\" folder, and runs write_results() on it (which in turn processes the files, acquires the\n",
    "    results, and writes the results out to a csv file).\n",
    "    \"\"\"\n",
    "    global res_dict\n",
    "    res_dict = dict()\n",
    "    for corpus in corpora:\n",
    "        res = write_results(corpus)\n",
    "        res_dict[corpus] = res\n",
    "    print(\"done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-level analysis done for corpus: lesbian-fanfic\n",
      "corpus-level analysis done for corpus: lesbian-fanfic\n",
      "results written out to csv for corpus: lesbian-fanfic\n",
      "--------------------\n",
      "file-level analysis done for corpus: lesbian-pulp\n",
      "corpus-level analysis done for corpus: lesbian-pulp\n",
      "results written out to csv for corpus: lesbian-pulp\n",
      "--------------------\n",
      "done!!\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lesbian-fanfic\n",
      "\n",
      "top 10 words\n",
      "[(10, 'like'), (10, 'back'), (9, 'just'), (9, \"don't\"), (8, \"i'm\"), (7, 'said'), (5, 'know'), (5, \"it's\"), (4, \"didn't\"), (3, 'eyes')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'she'\n",
      "[(11, 'be'), (10, 'have'), (9, 'say'), (8, 'do'), (7, \"didn't\"), (5, 'want'), (5, 'look'), (5, 'know'), (4, 'go'), (4, 'felt'), (3, 'whisper'), (3, 'take'), (3, 'get'), (3, \"couldn't\")]\n",
      "----------------------------------------\n",
      "top 10 verbs_'he'\n",
      "[(10, 'be'), (8, 'say'), (8, 'have'), (5, 'want'), (5, 'turn'), (5, 'go'), (5, 'get'), (5, 'do'), (5, \"didn't\"), (4, 'think'), (3, 'sigh'), (3, 'look')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'they'\n",
      "[(11, 'be'), (9, 'have'), (8, 'do'), (6, 'talk'), (5, 'make'), (5, 'go'), (5, 'get'), (4, 'think'), (3, 'want'), (3, 'walk'), (3, 'sit'), (3, 'kiss')]\n",
      "----------------------------------------\n",
      "top 10 coll - body\n",
      "[(8, 'around'), (5, 'upper'), (4, 'trembled'), (4, 'bottom'), (3, 'folded'), (3, 'curled')]\n",
      "----------------------------------------\n",
      "top 10 coll - intimacy\n",
      "[(4, \"i'd\")]\n",
      "----------------------------------------\n",
      "top 10 coll - identity\n",
      "[(3, 'assured')]\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "lesbian-pulp\n",
      "\n",
      "top 10 words\n",
      "[(11, 'said'), (10, 'like'), (10, \"don't\"), (9, 'know'), (5, 'laura'), (5, \"didn't\"), (4, 'time'), (4, 'thought'), (4, 'beebo'), (3, 'just'), (3, 'jack'), (3, \"i'm\"), (3, 'go'), (3, 'erika'), (3, 'beth')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'she'\n",
      "[(11, 'say'), (11, 'have'), (11, \"didn't\"), (11, 'be'), (10, 'think'), (9, 'look'), (9, 'know'), (9, 'felt'), (7, 'go'), (6, 'want'), (5, \"couldn't\")]\n",
      "----------------------------------------\n",
      "top 10 verbs_'he'\n",
      "[(11, 'say'), (11, 'look'), (11, 'have'), (11, 'be'), (7, 'think'), (7, \"didn't\"), (6, 'want'), (6, 'smile'), (6, 'know'), (6, 'do'), (5, 'go'), (4, 'turn'), (4, 'take'), (4, 'ask'), (3, 'get')]\n",
      "----------------------------------------\n",
      "top 10 verbs_'they'\n",
      "[(11, 'have'), (11, 'be'), (9, 'look'), (9, 'go'), (8, 'walk'), (7, 'sit'), (7, 'get'), (7, 'do'), (6, 'make'), (5, 'stand'), (4, 'take'), (3, 'meet'), (3, 'come')]\n",
      "----------------------------------------\n",
      "top 10 coll - body\n",
      "[(11, 'around'), (7, 'one'), (6, 'whole'), (4, 'long'), (4, \"laura's\"), (4, 'dark'), (4, 'black'), (3, \"erika's\"), (3, \"beth's\"), (3, \"beebo's\")]\n",
      "----------------------------------------\n",
      "top 10 coll - intimacy\n",
      "[(10, 'make'), (9, 'made'), (8, 'making'), (5, 'really'), (5, 'real'), (4, 'still'), (4, 'even'), (3, 'laura'), (3, 'could'), (3, 'beebo')]\n",
      "----------------------------------------\n",
      "top 10 coll - identity\n",
      "[(5, 'bar'), (4, 'pity'), (4, 'conscious'), (3, 'laura'), (3, \"i'm\"), (3, 'girl'), (3, 'consciously')]\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The functions in this block run some basic analytics on results to help me\n",
    "find inroads for my analysis. It's mostly just to help me and I wouldn't include it were I to share\n",
    "this code, but I am temporarily including it here.\n",
    "\"\"\"\n",
    "commonalities = dict()\n",
    "\n",
    "for text in res_dict[\"lesbian-fanfic\"]:\n",
    "    for result in text:\n",
    "        commonalities[result] = None\n",
    "        \n",
    "\n",
    "\n",
    "def read_csv(corpus_name):\n",
    "    filename = \"results_{}.csv\".format(corpus_name)\n",
    "    filepath = pathlib.Path(filename)\n",
    "    corpus_rows = list()\n",
    "    with open(filepath,\"r\") as file_obj:\n",
    "        content = file_obj.read()\n",
    "    rows = content.split(\"\\n\")\n",
    "    corpus_rows.extend(rows[0:-1])\n",
    "    return corpus_rows\n",
    "\n",
    "\n",
    "\n",
    "for corpus in corpora:\n",
    "    csv_dict = dict()\n",
    "    rows = read_csv(corpus)\n",
    "    csv_dict[\"corpus\"] = corpus\n",
    "    column_heads = rows[0].split(\",\")\n",
    "    relevant_headers = column_heads[1:2]+column_heads[3:6]+column_heads[9:]\n",
    "    for column_head in relevant_headers:\n",
    "        csv_dict[column_head] = None\n",
    "    for row in rows[1:]:\n",
    "        split_row = row.split(\",\")\n",
    "        split_row = split_row[1:6]+split_row[9:]\n",
    "        split_row.remove(split_row[1])\n",
    "        for column_ind in range(len(split_row)):\n",
    "            result = []\n",
    "            for word in split_row[column_ind].split():\n",
    "                result.append(word)\n",
    "            if csv_dict[relevant_headers[column_ind]] == None:\n",
    "                csv_dict[relevant_headers[column_ind]] = result\n",
    "            else:\n",
    "                csv_dict[relevant_headers[column_ind]].extend(result)\n",
    "    print(corpus+\"\\n\")\n",
    "    for relevant_header in relevant_headers:\n",
    "        if relevant_header != \"top 10 collocations\":\n",
    "            print(relevant_header)\n",
    "            counts = {}\n",
    "            counts_bynum = []\n",
    "            res_list = csv_dict[relevant_header]\n",
    "            for word in res_list:\n",
    "                if word not in counts:\n",
    "                    counts[word] = 1\n",
    "                else:\n",
    "                    counts[word] += 1\n",
    "            for key in counts:\n",
    "                if counts[key] >= 3:\n",
    "                    counts_bynum.append((counts[key],key))\n",
    "            counts_bynum.sort(reverse=True)\n",
    "            print(counts_bynum)\n",
    "            print(\"----------------------------------------\")\n",
    "    print(\"\\n\")\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "#     print(row_res[0].split(\",\"),\"\\n------\\n\",row_res[1])\n",
    "#     print(\"xxxxxxxxxx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter one\\nthe big house was still, almost empty. down the bright halls and in the\\nshadowy rooms everything was quiet. upstairs a few desk lights burned over\\npages of homework, but that was all.\\nthere was one room in the sorority house, however, where no reading was\\ngoing on. it was a big, warm room, meant for sprawling and studying and\\nsocializing in, like the others. three girls shared it and two of them were in it\\nnow on this autumn sunday night.\\none was a newcomer. her name was laura and she had just finished moving\\nall of her belongings into the room. it was a scene of overstuffed confusion,\\nbut at least she had somehow succeeded in squeezing all her things in and\\nnow there remained only the job of finding a place for them. laura sat down\\nto rest and worry about it. she tried to ignore the other girl.\\nbeth lay sprawled out on the studio couch with her head cushioned on a\\nrambling pile of fat pillows at one end and her feet dangling over the other.\\nshe was drinking a coke, resting the bottle on her stomach and letting it ride\\nthe rhythm of her breathing. she wore slim tan pants and a dark green\\nsweatshirt with \"alpha beta\" stamped in white on the front. her hair was\\ndark, curly, and close-cropped.\\nlaura sat by choice in the stiff wooden desk chair, as if beth were too\\ncomfortable and she could make amends by being uncomfortable herself. she\\nwas nervously aware of beth\\'s scrutiny, and the sorority pledge manual she\\nwas trying to read made no sense to her. the worst of it was that laura\\nwanted beth to like her. beth seemed like all good things to the younger girl\\'s\\ndazzled eyes: sophisticated, a senior, a leader, president of the student union,\\nand curiously pretty. she had a well-modeled, sensitive face with features not\\nbonily chic like those of a mannequin, but subtle, vital, harmonious. she\\nwasn\\'t fashionably pretty but her beauty was healthy and real and her good\\nnature showed in her face.\\nlaura flipped nervously through her pledge manual, not even pretending to\\nread any more. finally beth saw that she wasn\\'t reading and smiled at the\\nruse.\\n\"one hundred and thirty-seven pages of crap,\" she said, nodding at the\\nmanual. \"all guaranteed to confuse you. i don\\'t know why they don\\'t revise\\nthe damn thing. i\\'ve passed an exam on it and i still don\\'t understand it.\\'\\nher attitude embarrassed laura, who smiled uncertainly at her new\\nroommate, thinking as she did so how many times she had smiled in the same\\nway at beth, not sure of how she was expected to react.\\nshe had never known quite how to react to beth from the first day she had\\nseen her. it had been shortly after laura\\'s arrival at the university, when\\neverything she saw and felt excited her to a high pitch of nervous awareness.\\neven the sweet smoke of bonfires in the early-autumn air smelled new and\\ntantalizing.\\nlaura walked around the university town of champlain, down streets\\nchapeled with old elms; past the new campus with its clean, striking georgian\\nbuildings and past the old with its mellow moss-covered halls; past that copy\\nof the pantheon that passed for the auditorium; past the statues; past the\\nstudents walking down the white strip of the boardwalk, sitting on the steps\\nof buildings, stretching in the grass, and talking ... always talking.\\nit thrilled her, and it frightened her a little. some day she would know all of\\nthis as well as her home town; know the campus lore and landmarks, the\\ngreek alphabet, the football heroes, the habits of the campus cops. some day\\nshe wouldn\\'t have to ask the questionsâ€”she would be able to answer them. it\\nmade her feel a sort of grateful affection for the campus already, just to think\\nof it this way.\\nshe had been in school a week when she went up to the student union to join\\nan activity committee. it seemed like a good way to meet people and get into\\nthe university\\'s social life. laura had an appointment for an interview at three\\no\\'clock. she sat in the bustling student activities center on the third floor\\nwaiting to be called, clearing her throat nervously and sneaking a look at\\nherself in her compact mirror. she had a delicate face shaped like a thin white\\nheart, with startling pale blue eyes and brows and lashes paler still. a face\\nquaint and fine as a tenniel sketch.\\nshe waited for almost half an hour and the sustained anxiety began to tire her.\\nshe stared at her feet and up to the clock, and back to her feet again. it was\\nwhen she glanced at the clock for the last time that she saw beth for the first.\\nbeth was standing halfway across the room, tall and slender and with a\\nmagnetic face, talking to a couple of nodding boys. she was taller than one of\\nthem and the other acted as if she towered over him, too. laura watched her\\nwith absorbed interest. she tapped the smaller boy on the shoulder with a\\npencil as she talked to him and then she laughed at them both and laura\\nheard her say, \"okay, jack. thanks.\" she turned to leave them, coming\\nacross the room toward laura, and laura looked suddenly down at her shoes\\nagain. she told herself angrily that this was silly, but she couldn\\'t look up.\\nsuddenly she felt the light tap of a sheaf of papers on her head, and looked up\\nin surprise. beth smiled down at her. \"aren\\'t you new around here?\" she said,\\nlooking at laura with wide violet eyes.\\n\"yes,\" laura said. her throat was dry and she tried to clear it again.\\n\"are you on a committee?\"\\nshe was strangely, compellingly pretty, and she was looking down at laura\\nwith a frank, friendly curiosity that confused the younger girl.\\n\"i\\'m here for an interview,\" laura said in a scratchy voice.\\nbeth waited for her to say something more and laura felt her cheeks\\ncoloring. a young man thrust his face out of a nearby door and said, \"laura\\nlandon?\" looking around him quizzically.\\n\"here.\" laura stood up.\\n\"oh. come on in. we\\'re ready for you.\" he smiled.\\nbeth smiled, too. \"good luck,\" she said, and walked away.\\nlaura looked after her, until the boy said, \"come on in,\" again.\\n\"oh,\" she said, whirling around, and then she smiled at him in\\nembarrassment. \"sorry.\"\\nthe interview turned out well. laura joined the campus chest committee and\\nturned her efforts toward parting students from their allowances for good\\ncauses. every afternoon she went up to the union building and put in an hour\\nor two in the campus chest office on the third floor, where most of the major\\ncommittees had offices.\\nit had been nearly two weeks later that beth stopped in the office to talk to\\nthe chairman. she sat on his desk and laura, carefully looking at a paper in\\nfront of her, listened to every word they said. it was mostly business:\\ncommittee work, projects, hopes for success. and then the chairman told her\\nwho was doing the best work for campus chest. he named three or four\\nnames. beth nodded, only half listening.\\n\"and laura landon\\'s done a lot for us,\" he said.\\n\"um-hmm,\" said beth, taking little notice. she was gathering her papers,\\nabout to leave.\\n\"hey, laura.\" he waved her over.\\nlaura got up and came uncertainly toward the desk. beth straightened her\\npapers against the top of the desk, hitting them sideways the long way and\\nthen the short way until all the edges were even.\\nbeth, this is laura landon,\" the boy said.\\nbeth looked up and smiled. and then her smile broadened. \"oh, you\\'re laura\\nlandon,\" she said. she held out her hand. \"hi, laur.\"\\nnobody had ever called her \"laur\" before; she wasn\\'t the type to inspire\\nnicknames. but she liked it now. she took beth\\'s hand. \"hi,\" she said.\\n\"you know each other?\" the chairman said.\\n\"we\\'ve never had a formal introduction,\" beth said, \"but we\\'ve had a few\\nwords together.\" laura remained silent, a little desperate for conversation.\\n\"well, then,\" said the chairman gallantly, \"miss cullison, may i present miss\\nlandon.\\n\"will miss landon have coffee with miss cullison this afternoon?\" said\\nbeth.\\nlaura smiled a little. \"she\\'d be delighted,\" she said.\\nthey did. and she was. an occasional fifteen- or thirty-minute coffee break\\nwas traditional at the union building. beth and laura went down to the\\nbasement coffee shop, and came up two hours later because it was time\\nfinally to go home for dinner. laura couldn\\'t remember exactly what they\\ntalked about. she recalled telling beth where she was living and what she\\nwas studying. and she remembered a long monologue from beth on the\\nstudent union activities and what they accomplished. and then suddenly\\nbeth had said, \"are you going to go through rushing, laur?\\n\"rushing?\"\\n\"yes. to join a sorority. informal rush opens next week.\"\\n\"well, iâ€”i hadn\\'t thought about it.\"\\n\"think about it, then. you should, laura. i\\'m on alpha beta and, strictly off\\nthe record, i think we\\'d be very interested.\"\\n\"why would alpha beta want me?\" laura said to her coffee cup.\\n\"because i think it\\'s a good idea. and alpha beta listens to beth cullison.\"\\nshe laughed a little at herself. \"does that sound hopelessly egotistical? it\\ndoes, doesn\\'t it? but it\\'s true.\" she paused, waiting until laura looked at her\\nagain. \"sign up for rushing, laura,\" she said, \"and i\\'ll see to it you\\'re\\npledged.\"\\n\"iâ€”i will. i certainly will, beth,\" laura said, hardly daring to believe what\\nshe\\'d heard.\\nbeth grinned. \"my god, it\\'s nearly five-thirty,\" she said. \"let\\'s go.\"\\nafter that it had been easy. beth spoke the truth; alpha beta did listen to her.\\nlaura had signed up for rush, with the secret understanding that she would\\npledge alpha beta. but even at that, it was a thrill when beth called her two\\ndays after rushing was over and said, \"hi, honey. pack your things. you\\'re an\\nalpha beta now. officially.\\nlaura had cried over the phone, and beth said, \"you don\\'t have to, you\\nknow.\"\\n\"but i want to!\"\\nbeth laughed. \"okay, laur, come on over. you just joined one of the world\\'s\\nmost exclusive clubs. and you have a new roommate. in fact you have two.\"\\n\"two?\"\\n\"yes. me. and emily.\"\\nemily had spent the day with them, helping laura bring things in and put\\nthem away. laura was so tired now she could hardly recall emily\\'s face; all\\nshe remembered was a warm, ready laugh and the vague impression that\\nemily was fashioned to please the fussiest males: the ones who want perfect\\nlooks and perfect compliance in a woman.\\nbeth had called a halt to their work early in the evening.\\n\"we\\'ve done enough, laura,\" she had said, dropping down on the studio\\ncouch. \"we\\'ve even done too much.\\n\"it was wonderful of you to help me, beth.\"\\n\"oh, i know. i\\'m wonderful as all hell. i only did it because i had to.\" she\\ngrinned at laura, who smiled self-consciously back. beth liked to tease her\\nfor being too polite and it made laura uncomfortable. she would have gone\\nto almost any length to please beth, and yet she could not abandon her good\\nmanners. they struck her as one of her best features, and it puzzled her that\\nbeth should needle her about them. she knew beth could carry off a courtesy\\nbeautifully at the right moment; laura had seen her do it. but beth was much\\nless formal than her new roommate, and furthermore she liked to swear,\\nwhich laura thought extremely unmannerly. beth made laura squirm with\\ndiscomfort. and in self-defense laura tried to build a wall of politeness\\nbetween them, to admire beth from faraway.\\nthere was a vague, strange feeling in the younger girl that to get too close to\\nbeth was to worship her, and to worship was to get hurt. as yet, beth made\\nno sense to her, she fit no mold, and laura wanted to keep herself at an\\nemotional distance from her. she had never met or read or dreamed a beth\\nbefore and until she could understand her she would be afraid of her.\\nlaura had been thinking about this that afternoon while she filled the drawers\\nof her new dresser with underwear and sweaters and scarves and socks, and\\nhad resolved right then that she must always be on her guard with beth. she\\ndidn\\'t know what she was trying to shield herself from; she only felt that she\\nneeded protection somehow.\\nbeth had suddenly put an arm around her shoulders, shaking the thoughts out\\nof her head, and said with a laugh, \"for god\\'s sake, laur, how many pairs of\\npanties do you have? look at \\'em all, emmy.\"\\nand emily had looked up and laughed pleasantly. laura couldn\\'t tell if she\\nwere laughing at the underwear or at beth or at the look on laura\\'s face, for\\nlaura looked as surprised as she was. she stood there for a minute, feeling\\nonly the weight and pull of beth\\'s arm and not the necessity to answer.\\nin a faint voice laura answered, \"my mother buys all my underwear. she\\ngets it at field\\'s.\"\\n\"well, she must\\'ve cleaned them out this time,\" said beth, smiling at the\\nluxurious drawerful. \"i\\'ll bet they put in an emergency order for undies when\\nshe leaves the store.\"\\nemily laughed again and laura shut the drawer with a smack and cleared her\\nthroat. she hated to talk about lingerie. she hated to undress in front of\\nanyone. she even hated to wash her underwear because she had to hang it on\\nthe drying racks in the john or in the laundry room where everyone could see\\nit. it was no comfort to her that everybody else did the same thing.\\n\"of course, i don\\'t believe in underwear myself,\" said beth airily. \"never\\nwear any.\" she swept a stack of sweaters theatrically off the table and handed\\nthem to laura, who gazed at her in dismay, reaching mechanically for the\\nsweaters. beth laughed. \"i\\'m pretty wicked, laur.\"\\n\"don\\'t you really wear anyâ€”any underwear?\" her whole upbringing revolted\\nat this. \"you must wear some.\"\\nbeth shook her head, enjoying laura\\'s distress and surprised at how little it\\ntook to shock her. laura looked at her with growing outrage until she burst\\nout laughing and emily intervened sympathetically.\\n\"beth, you\\'re going to make your poor little roommate think she\\'s fallen in\\nwith a couple of queers,\" she said with a giggle.\\nbeth grinned at laura and the younger girl felt strangely as if the bottom had\\nfallen out of her stomach.\\n\"she has,\" said beth with emphatic cheerfulness. \"she ought to know the\\ndreadful truth. we\\'re characters, laura. desirable characters, of course, but\\nstill characters. are you with us?\"\\nlaura wished for a moment that she were all alone in a vacuum. she didn\\'t\\nknow whether to take beth seriously or not; she felt as if beth were testing\\nher, challenging her, and she didn\\'t know how to meet the challenge. she\\ntransferred a sweater nervously from one hand to the other and tried to\\nanswer. nobody was a more rigid conformist, farther from a character, than\\nlaura landon. but the bothersome need to please beth prompted her to say\\nweakly, \"yes.\"\\nshe put the sweater in a drawer, turning away from beth and emily as she\\ndid so, and silently and secretly scraped the white undersides of her forearms.\\nit was an old gesture. whenever she was disappointed with herself she\\nbruised herself physically. the sad red lines she raised on her skin were her\\nexpiation, a way of squaring with herself.\\nbeth, who could see she had gone far enough, confined herself for a while to\\nfriendly suggestions and answering questions. it was a great relief to laura.\\nshe was almost herself again when beth suggested a tour of the sorority\\nhouse.\\nthe two girls went first up to the dormitory on the third floor, where\\neverybody but the housemother and the household help slept.\\n\"does anyone ever sleep in the rooms?\" laura asked as they mounted the\\nstairs.\\n\"oh, once in a while. in the winter, when the dorm is really cold, some of the\\nkids sleep in their rooms. the studio couches unfold into double beds. they\\ncan sleep two.\"\\nthey had entered the big quiet dorm with its dozens of iron bunk beds\\nsmothered in comforters and down pillows and bright blankets. laura\\nshivered in the chill while beth pointed out her unmade bed to her.\\n\"we\\'ll have to come back and make it up later,\" she said.\\nbeth had then led laura down to the basement. she was enjoying this new\\nrole of guide and guardian, enjoying even more laura\\'s unquestioning\\nacceptance of it. they found themselves playing a pleasant little game\\nwithout ever having to refer to the rules: when they reached the door to the\\nback stairs together, laura stopped, as if automatically, and let beth hold the\\ndoor for her. laura, who tried almost instinctively to be more polite than\\nanybody else, readily gave up all the small faintly masculine courtesies to\\nbeth, as if it were the most natural thing in the world, as if beth expected it of\\nher. there was no hint that such an agreeable little game could suddenly turn\\nfast and wild and lawless.\\nin the basement beth showed her the luggage room, shelved to the ceiling\\nand crowded with all manner of plaid and plastic and leather cases. in the rear\\nof the room was a closed door.\\nbeth turned around to go out and bumped softly into laura, who had been\\nwaiting for an explanation of the closed door. laura jumped back and beth\\nsmiled slowly and said, \"i won\\'t eat you, laur.\"\\nlaura felt a crazy wish to turn and run, but she held her ground, unable to\\nanswer.\\nbeth put her hands gently on laura\\'s shoulders. \"are you afraid of me, laur?\\nshe said. there was a long, terribly bright and searching silence.\\n\"iâ€”i wondered what the door in back was to,\" laura faltered. her sentence\\nseemed to hang suspended, without a period.\\nbeth let her hands drop. \"that\\'s the chapter room,\" she said. \"verboten. until\\nyou\\'re initiated, of course.\"\\n\"oh,\" said laura, and she walked out of the luggage room with beth\\'s strange\\nsmile wreaking havoc in the pit of her stomach.\\non the way upstairs they met mary lou baker, the president of alpha beta.\\nshe came down the stairs toward them, towing a bulging bundle of laundry\\nwhich bumped dutifully down the stairs behind her. she smiled at them and\\nsaid, \"hi there. how\\'s the unpacking coming along, laura?\"\\n\"fine, thank you.\" laura watched mary lou retreat into the basement,\\nimpressed with her importance.\\n\"she likes you,\" said beth as they headed back up to their room.\\n\"she does?\" laura smiled, pleasantly surprised.\\n\"um-hmm,\" beth answered. usually she has nothing to say to newcomers for\\na few weeks. if she notices you right away it\\'s a good sign. at least it is if\\nyou\\'re interested in her approval.\" she said this rather disparagingly.\\nwalking down the hall behind her, laura smiled.\\nand now here they were in the calm of a sunday night, alone in their room,\\ncurious and shy at the same time. beth finished her coke and set the bottle\\ndown on a glass-topped coffee table in front of the studio couch. the clack of\\nglass on glass startled laura and the pledge manual slipped from her hands to\\nthe floor.\\n\"want to go make your bed up now?\" beth said. her voice was soft, as if she\\nwere rather tired.\\n\"oh, yes. i guess i\\'d better.\"\\n\"i\\'ll help you.\" beth sat up, swinging her long legs to the floor. she sat still\\nfor a minute as if getting her bearings, looking at her feet. then she lit a\\ncigarette. \"come on, let\\'s go do it,\" she said finally with sudden brightness.\\n\"i\\'ll do it, beth,\" said laura firmly. \"you\\'ve done so much for me today, i\\njust hate to have you do any more.\"\\nbeth blew smoke over the table top. \"laura, if you don\\'t stop thanking me for\\neverything you\\'re going to wear me out,\" she said. \"or turn my head.\" she\\nsaid this good-naturedly, to tease more than to scold. but then she saw that\\nshe had hurt laura and she wanted instantly to reach out with comfort and\\nreassurance. she was not impatient with laura\\'s hypersensitivity, only unused\\nto it. she never knew when she might scrape against it and cause pain.\\nlaura\\'s mouth tightened and she gripped the cover of her pledge manual in an\\neffort to calm herself.\\n\"laura,\" said beth in a gentle voice, and she got up and went over to her.\\nlaura drew back in surprise as beth dropped to her knees in front of the\\nchair, putting a hand on laura\\'s knees and smiling up at her. laura was too\\nstartled to pretend composure.\\n\"laur, have i hurt your feelings, honey? i have, haven\\'t i? answer me.\"\\nlaura said helplessly, \"no, beth, reallyâ€”\"\\n\"i know i have,\" beth interrupted her. \"i\\'m sorry, laur. you mustn\\'t take me\\nso seriously. i\\'m only teasing. i like to tease, but i don\\'t like to hurt people.\\nyou just have to get used to me, that\\'s all. take me with a grain of salt.\" she\\nlooked earnestly at her with the shade of a smile on her lips and she thought\\nhow good it would be to skid her hands hard up laura s thighs and ... so she\\nkept talking. it was better to ignore the peculiar feelings laura awoke in her;\\nshe covered her confusion with words.\\n\"because i want us to be good friends,\" she went on. \"and i\\'ll try not toâ€”to\\nshock you any more. i guess i\\'m a little crazyâ€”the results of a misspent\\nyouth, or course.\" and she grinned. \"but i\\'m not dangerous, honest to god.\\nnowâ€”\" she smacked laura\\'s knees amiablyâ€”\"we\\'re over the first crisis. are\\nwe going to be friends, laur?\"\\nlaura wanted desperately to pull her knees together. \"yes,\" she said to beth.\\n\"i hope so.\"\\n\"good!\" said beth and she bounced to her feet. \"come along, then. let\\'s\\nmake your bed.\"\\nit hadn\\'t taken long to make up the austere box bed and laura found herself\\nback in the room and faced with the humiliating problem of undressing in\\nfront of somebody else. her shyness settled in her cheeks and neck like a heat\\nrash. as soon as she felt the burn, it spread to her shoulders and bosom. she\\nblushed very easily and she despised herself for it. she wanted to scratch at\\nher arms again, but because beth would notice it she had to content herself\\nwith biting the tender flesh of her underlip until she was afraid it would bleed\\nand cause her more grief.\\nshe turned as far from beth as she could and unbuttoned her blouse,\\nsomehow feeling that beth\\'s bright eyes were doting on every button. but\\nbeth was subtle; she was humming a tune and busy with her pajamas. she\\nsaw laura without seeming to and laura began to envy her pleasant abandon.\\nafter a moment she said, \"laur, do you have a sweatshirt?\"\\n\"yes.\" laura eyed her quizzically.\\n\"better put it on. the dorm is a damn deep freeze.\"\\nlaura found the sweatshirt and pulled it over her head, and beth led her up to\\nthe dorm. on the door was posted a wake-up chart with a pencil on a string\\nhanging beside it. beth signed laura\\'s name under \"6:45.\"\\n\"think you can find your bed?\" she asked.\\n\"there it is,\" said laura, pointing.\\n\"okay, in you go,\" said beth.\\nlaura studied the upper bunk, which looked unattainable. \"how?\" she\\nfaltered.\\nbeth laughed quietly. \"well, look,\" she said. \"put your foot on the rung of\\nthe lower bunkâ€”no, no, wait!â€”that\\'s right,\" she said, guiding her. \"now, get\\nyour knee on the rung of the bed next door. now, just roll in. whoops!\" she\\nsaid, catching laura as she nearly lost her balance. she gave her a push in the\\nright direction. laura rolled awkwardly onto her bunk, laughing with beth.\\nbeth climbed up where she could see her and said, \"you\\'ll catch on, laur.\\ndoesn\\'t take long.\" she helped laura under the covers and tucked her in, and\\nit was so lovely to let herself be cared for that laura lay still, enjoying it like\\na child. when beth was about to leave her, laura reached for her naturally,\\nlike a little girl expecting a good-night kiss. beth bent over her and said,\\n\"what is it, honey?\"\\nwith a hard shock of realization, laura stopped herself. she pulled her hands\\naway from beth and clutched the covers with them.\\n\"nothing.\" it was a small voice.\\nbeth pushed laura\\'s hair back and gazed at her and for a heart-stopping\\nmoment laura thought she would lean down and kiss her forehead. but she\\nonly said, \"okay. sleep tight, honey.\" and climbed down.\\nlaura raised herself cautiously on one elbow so she could watch her leave the\\ndorm. beth went out and shut the door and laura was left to her strange cold\\nbed in the great dark dormitory. she felt cut loose from reality.\\nit took her a long while to get to sleep. her nerves were brittle as ice and they\\nall seemed to be snapping from the day\\'s pressure. she lay motionless on her\\nback and studied the luminous checkers on the ceiling, laid there through the\\nwindow by the light of the fire escape. she thought of beth: beth beside her\\nwatching her, whispering to her, reaching put to touch her.\\nthe stillness grew and lengthened and laura lay in it alone with her thoughts.\\nfar away on the campus the clock on the student union steeple pulsed twelve\\ntimes through the waiting night. laura pulled her covers tight under her chin\\nand tried to sleep. she was just drifting off when she heard someone stop by\\nher bed and she opened her heavy eyes and saw beth outlined by the night\\nlight.\\n\"still awake?\" she whispered.\\n\"i\\'m sorry. i\\'m dropping off now.\" laura felt guilty; caught with her eyes\\nopen when they should have been shut; caught peeking at nothing; caught\\nthinking of beth.\\n\"just wanted to make sure you were all right.\"\\n\"oh yes, thank you.\"\\n\"shhh!\" hissed someone from a neighboring bed.\\n\"sorry!\" beth hissed back, and then turned to laura again. \"okay, go to sleep\\nnow,\" she said, and she gave laura\\'s arm a pat.\\n\"i will,\" laura whispered.\\nchapter two', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "working_doc = \"data/lesbian-pulp/bannon_odd-girl-out.txt\"\n",
    "with open(working_doc,\"r\",encoding=\"utf-8\") as f:\n",
    "    txt = f.read().strip().lower()\n",
    "    \n",
    "def segmenter(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    chapter_pattern = \"chapter [a-z0-9]+(?=\\n)\"\n",
    "    ch_res = re.findall(chapter_pattern,text)\n",
    "    counter = 0\n",
    "    matches = list()\n",
    "    while counter < len(ch_res):\n",
    "        if counter < len(ch_res) - 1:\n",
    "            chapter_text_pattern = \"{}[\\s\\S]+{}\".format(ch_res[counter],ch_res[counter+1])\n",
    "            chapter_text = re.match(chapter_text_pattern,text)\n",
    "            try:\n",
    "                matches.append(chapter_text.group())\n",
    "            except:\n",
    "                matches.append(chapter_text)\n",
    "        else:\n",
    "            chapter_text_pattern = \"{}[\\s\\S]+\".format(ch_res[counter])\n",
    "            chapter_text = re.match(chapter_text_pattern,text)\n",
    "            matches.append(chapter_text)\n",
    "        counter += 1\n",
    "    print(matches)\n",
    "\n",
    "    \n",
    "segmenter(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
